{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fcb4bf2-cd0b-4b25-8159-705e68d3a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./bulkdata/final_descriptions_with_scores.csv\")\n",
    "\n",
    "# ìµœì¢… DataFrameì—ì„œ description ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "final_description_df = df[['description']]\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ì¥ (íŒŒì¼ ê²½ë¡œëŠ” í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)\n",
    "final_description_df.to_csv(\"./bulkdata/descriptions.csv\", index=False)\n",
    "\n",
    "print(\"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3cb5f6-dd0c-4773-925c-6fa28b6b5730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            description  PN\n",
      "0         â„ ì—ì–´ì»¨ì„ 26ë„ë¡œ ì„¤ì •í•˜ê³  í•„ìš”í•œ ì‹œê°„ì—ë§Œ ì¼ ë‹¤.   1\n",
      "1             ğŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.  -1\n",
      "2          ğŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.   1\n",
      "3              ğŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.  -1\n",
      "4            ğŸ”Œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì „ìì œí’ˆì˜ í”ŒëŸ¬ê·¸ë¥¼ ë½‘ì•„ë‘”ë‹¤.   1\n",
      "5          ğŸ¥˜ ìŒì‹ë¬¼ ì°Œêº¼ê¸°ë¥¼ ë¬¼ë¡œ ì”»ì–´ í•˜ìˆ˜êµ¬ì— í˜ë ¤ë³´ë‚¸ë‹¤.  -1\n",
      "6          ğŸ’§ ì„¸ì œë¥¼ ì‚¬ìš©í•œ ê±¸ë ˆë¥¼ íë¥´ëŠ” ë¬¼ë¡œ ì˜¤ë˜ í—¹êµ°ë‹¤.  -1\n",
      "7       ğŸ’§ ê¸°ë¦„ ë¬»ì€ í›„ë¼ì´íŒ¬ì„ í‚¤ì¹œíƒ€ì›”ë¡œ ë‹¦ì€ í›„ ì„¤ê±°ì§€í•œë‹¤.   1\n",
      "8                 ğŸ—‘ í™”ì¥ì‹¤ì— ë¬¼í‹°ìŠˆë‚˜ ê¸°ì €ê·€ë¥¼ ë²„ë¦°ë‹¤.  -1\n",
      "9      ğŸ’§ ë‚¨ì€ ì•½ì´ë‚˜ íê¸°ë¬¼ì„ ì•½êµ­ ë˜ëŠ” ì§€ì • ìˆ˜ê±°í•¨ì— ë²„ë¦°ë‹¤.   1\n",
      "10               ğŸ›¢ ìš”ë¦¬ ì¤‘ ëƒ„ë¹„ ëšœê»‘ì„ ë®ê³  ì¡°ë¦¬í•œë‹¤.   1\n",
      "11          ğŸ”¥ ê²¨ìš¸ì²  ë³´ì¼ëŸ¬ë¥¼ í•˜ë£¨ ì¢…ì¼ í‹€ì–´ë†“ê³  ì™¸ì¶œí•œë‹¤.  -1\n",
      "12     ğŸ”¥ ë³´ì¼ëŸ¬ ì˜¨ë„ëŠ” ë‚®ì¶”ê³ , ì˜¨ìˆ˜ëŠ” í•„ìš”í•œ ë§Œí¼ë§Œ ì‚¬ìš©í•œë‹¤.   1\n",
      "13         ğŸ›¢ ê°€ìŠ¤ë ˆì¸ì§€ì— ë¶ˆê½ƒì´ ëƒ„ë¹„ ë°”ê¹¥ìœ¼ë¡œ í¼ì§€ê²Œ ì¼ ë‹¤.   1\n",
      "14  ğŸ›¢ ê°€ìŠ¤ ëˆ„ì¶œ ì ê²€ì„ ì •ê¸°ì ìœ¼ë¡œ í•˜ê³ , ì˜¤ë˜ëœ í˜¸ìŠ¤ë¥¼ êµì²´í•œë‹¤.   1\n",
      "15         ğŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.   1\n",
      "16            ğŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.  -1\n",
      "17                        ğŸ§º ë§¤ì¼ ì„¸íƒê¸°ë¥¼ ëŒë¦°ë‹¤   1\n",
      "18             ğŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.  -1\n",
      "19                      ğŸ’¡ ë¶ˆí•„ìš”í•œ ì¡°ëª…ì„ êº¼ë‘”ë‹¤.  -1\n",
      "20                      â„ ëƒ‰ì¥ê³  ë¬¸ì„ ìì£¼ ì—°ë‹¤.   1\n",
      "21                â„ ì—ì–´ì»¨ ì˜¨ë„ë¥¼ í•­ìƒ ë‚®ê²Œ ìœ ì§€í•œë‹¤.  -1\n",
      "22                       ğŸ”Œ ì»´í“¨í„°ë¥¼ í•­ìƒ ì¼œë‘”ë‹¤.  -1\n",
      "23                   ğŸ›¢ ìš”ë¦¬ ì‹œ ëƒ„ë¹„ ëšœê»‘ì„ ë®ëŠ”ë‹¤.   1\n",
      "24              ğŸ”¥ ë¬¼ì„ ë“ì¼ ë•Œ í•­ìƒ ìµœëŒ€ ë¶ˆë¡œ ë“ì¸ë‹¤.   1\n",
      "25                 ğŸ›¢ ê°€ìŠ¤ë¥¼ ì¼œë‘” ì±„ ë‹¤ë¥¸ ì¼ì„ í•œë‹¤.   1\n",
      "26                   ğŸ›¢ ë§¤ì¼ ì˜¤ëœ ì‹œê°„ ë‚œë°©ì„ í•œë‹¤.   1\n",
      "27                      ğŸ’§ ì–‘ì¹˜ì§ˆ ì‹œ ë¬¼ì„ ì ê·¼ë‹¤.   1\n",
      "28                       ğŸ’§ ëª©ìš•íƒ•ì„ ë§¤ì¼ ì±„ìš´ë‹¤.   1\n",
      "29                        ğŸ’§ ë¬¼ì„ ë§ì´ ì‚¬ìš©í•œë‹¤.   1\n",
      "30               ğŸ’§ ì„¸ì°¨í•  ë•Œ í•­ìƒ ë¬¼ì„ ê³„ì† í‹€ì–´ë‘”ë‹¤.   1\n",
      "31           ğŸ”¥ ë‚œë°©ì„ ì ì ˆíˆ ë‚®ì¶”ê³  ì˜·ì„ ë”°ëœ»í•˜ê²Œ ì…ëŠ”ë‹¤.   1\n",
      "32                      â„ ì°½ë¬¸ì„ í•­ìƒ ì—´ì–´ ë‘”ë‹¤.   1\n",
      "33                  ğŸ›¢ ì§‘ ì „ì²´ë¥¼ í•˜ë£¨ ì¢…ì¼ ë‚œë°©í•œë‹¤.  -1\n",
      "34               ğŸ›¢ ë‚œë°©ê¸°ë¥¼ í•­ìƒ ìµœê³  ì˜¨ë„ë¡œ ì„¤ì •í•œë‹¤.   1\n",
      "35                      ğŸ—‘ ë‹¤íšŒìš© ë¬¼ë³‘ì„ ì‚¬ìš©í•œë‹¤.   1\n",
      "36                    ğŸ—‘ ì¼íšŒìš© ì»µì„ ìì£¼ ì‚¬ìš©í•œë‹¤.   1\n",
      "37              ğŸ—‘ í”Œë¼ìŠ¤í‹± í¬ì¥ëœ ì œí’ˆì„ ìì£¼ êµ¬ë§¤í•œë‹¤.   1\n",
      "38                     ğŸ—‘ ë¹„ë‹ë´‰ì§€ë¥¼ ìì£¼ ì‚¬ìš©í•œë‹¤.   1\n",
      "39                       ğŸ—‘ í•„ìš”í•œ ì–‘ë§Œ ì¡°ë¦¬í•œë‹¤.   1\n",
      "40                    ğŸ¥˜ í•­ìƒ ìŒì‹ì„ ë„‰ë„‰íˆ ë§Œë“ ë‹¤.   1\n",
      "41                        ğŸ¥˜ ìŒì‹ì„ ìì£¼ ë²„ë¦°ë‹¤.  -1\n",
      "42                  ğŸ¥˜ ë‚¨ì€ ìŒì‹ ë³´ê´€ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.   1\n",
      "43                   ğŸš² ìì „ê±° ë˜ëŠ” ê±·ê¸°ë¥¼ ì´ìš©í•œë‹¤.   1\n",
      "44                        ğŸš— ì°¨ë¥¼ ìì£¼ ì´ìš©í•œë‹¤.   1\n",
      "45                  ğŸš— ê°€ê¹Œìš´ ê±°ë¦¬ë„ ìë™ì°¨ë¡œ ë‹¤ë‹Œë‹¤.   1\n",
      "46                   ğŸš— ëŒ€ì¤‘êµí†µ ì´ìš©ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.   1\n",
      "47                         ğŸ—‘ ì–‘ë©´ ì¸ì‡„ë¥¼ í•œë‹¤.   1\n",
      "48                   ğŸ—‘ ì¢…ì´ë¥¼ ì¼íšŒìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•œë‹¤.   1\n",
      "49                       ğŸ—‘ ì¢…ì´ë¥¼ ë§ì´ ì‚¬ìš©í•œë‹¤.   1\n",
      "50                        ğŸ—‘ í•œìª½ ë©´ë§Œ ì¸ì‡„í•œë‹¤.   1\n",
      "51          ğŸŒ³ ëŒ€ê¸°ì˜¤ì—¼ ê°ì†Œì™€ ì—ë„ˆì§€ë¥¼ ì¤„ì´ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.   1\n",
      "52                ğŸš— ì£¼ì°¨ ê³µê°„ ë¶€ì¡± ë¬¸ì œë¥¼ ì‹¬í™”ì‹œí‚¨ë‹¤.   1\n",
      "53      ğŸš— ë„ë¡œ í™•ì¥ì„ ìœ ë°œí•˜ì—¬ í™˜ê²½ íŒŒê´´ ì›ì¸ì´ ë  ìˆ˜ ìˆë‹¤.   1\n",
      "54       ğŸ›¢ í™”ì„ ì—°ë£Œ ì‚¬ìš©ì„ ê°•ì œí•´ ì§€êµ¬ ì˜¨ë‚œí™”ë¥¼ ê°€ì†í™”í•œë‹¤.   1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "import io\n",
    "\n",
    "# 1. í•™ìŠµ ë°ì´í„° ì¬ì •ì˜ (ì‚¬ìš©ì ë ˆì´ë¸” ë° í”¼ë“œë°±)\n",
    "user_labels_data = \"\"\"\n",
    "â„ ì—ì–´ì»¨ì„ 26ë„ë¡œ ì„¤ì •í•˜ê³  í•„ìš”í•œ ì‹œê°„ì—ë§Œ ì¼ ë‹¤.,1\n",
    "ğŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.,-1\n",
    "ğŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.,1\n",
    "ğŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.,-1\n",
    "ğŸ”Œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì „ìì œí’ˆì˜ í”ŒëŸ¬ê·¸ë¥¼ ë½‘ì•„ë‘”ë‹¤.,1\n",
    "ğŸ¥˜ ìŒì‹ë¬¼ ì°Œêº¼ê¸°ë¥¼ ë¬¼ë¡œ ì”»ì–´ í•˜ìˆ˜êµ¬ì— í˜ë ¤ë³´ë‚¸ë‹¤.,-1\n",
    "ğŸ’§ ì„¸ì œë¥¼ ì‚¬ìš©í•œ ê±¸ë ˆë¥¼ íë¥´ëŠ” ë¬¼ë¡œ ì˜¤ë˜ í—¹êµ°ë‹¤.,-1\n",
    "ğŸ’§ ê¸°ë¦„ ë¬»ì€ í›„ë¼ì´íŒ¬ì„ í‚¤ì¹œíƒ€ì›”ë¡œ ë‹¦ì€ í›„ ì„¤ê±°ì§€í•œë‹¤.,1\n",
    "ğŸ—‘ í™”ì¥ì‹¤ì— ë¬¼í‹°ìŠˆë‚˜ ê¸°ì €ê·€ë¥¼ ë²„ë¦°ë‹¤.,-1\n",
    "ğŸ’§ ë‚¨ì€ ì•½ì´ë‚˜ íê¸°ë¬¼ì„ ì•½êµ­ ë˜ëŠ” ì§€ì • ìˆ˜ê±°í•¨ì— ë²„ë¦°ë‹¤.,1\n",
    "ğŸ›¢ ìš”ë¦¬ ì¤‘ ëƒ„ë¹„ ëšœê»‘ì„ ë®ê³  ì¡°ë¦¬í•œë‹¤.,1\n",
    "ğŸ”¥ ê²¨ìš¸ì²  ë³´ì¼ëŸ¬ë¥¼ í•˜ë£¨ ì¢…ì¼ í‹€ì–´ë†“ê³  ì™¸ì¶œí•œë‹¤.,-1\n",
    "\"\"\"\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "user_df = pd.read_csv(io.StringIO(user_labels_data), header=None, names=[\"description\", \"score\"])\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ê²°í•© í›„, NaN ê°’ ì œê±°\n",
    "updated_train_data = updated_train_data.dropna(subset=[\"user_score\"])\n",
    "\n",
    "# 2. TF-IDF ë²¡í„°í™” ë° Ridge íšŒê·€ ëª¨ë¸ ì¬í•™ìŠµ\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(updated_train_data[\"description\"])\n",
    "y_train = updated_train_data[\"user_score\"].astype(float)\n",
    "\n",
    "reg_model = Ridge()\n",
    "reg_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° ë° ëª¨ë¸ ì˜ˆì¸¡\n",
    "df_reloaded = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# description ì¹¼ëŸ¼ ê¸°ë°˜ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "X_all = vectorizer.transform(df_reloaded[\"description\"])\n",
    "reg_predictions = reg_model.predict(X_all)\n",
    "\n",
    "# ì´ì§„ ê¸/ë¶€ì • íŒë‹¨: 0ë³´ë‹¤ í¬ë©´ 1(ê¸ì •), ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ -1(ë¶€ì •)\n",
    "df_reloaded[\"PN\"] = [1 if score > 0 else -1 for score in reg_predictions]\n",
    "\n",
    "# ê²°ê³¼ ë¯¸ë¦¬ ë³´ê¸°\n",
    "print(df_reloaded.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b07e429-2e27-463c-8b62-0fdd032bdc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì ìš© ì™„ë£Œ: descriptions_with_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ê¸ì • ë‹¨ì–´ 100ê°œ (ê° ë‹¨ì–´ëŠ” ìµœëŒ€ 3ê¸€ì)\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ì¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ì¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ì¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ì\", \"ì •ì±…\",\n",
    "    \"ìì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ì¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìí™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìê²½\", \"ì¹œì\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìì§€\", \"ì—ê²½\", \"ì—ì •\", \"ì¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìí•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "# ë¶€ì • ë‹¨ì–´ 100ê°œ (ê° ë‹¨ì–´ëŠ” ìµœëŒ€ 3ê¸€ì)\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ì¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìí˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ì¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë‚´ì— í¬í•¨ëœ ê¸ì • ë‹¨ì–´ì™€ ë¶€ì • ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# ê° ë¬¸ì¥ì— ëŒ€í•´ ì‚¬ì „ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (lex_score)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words_100, negative_words_100))\n",
    "\n",
    "# ê²°ê³¼ CSV íŒŒì¼ë¡œ ì €ì¥ (ì˜ˆ: descriptions_with_lexicon.csv)\n",
    "df.to_csv(\"./bulkdata/descriptions_with_lexicon.csv\", index=False)\n",
    "\n",
    "print(\"ì ìš© ì™„ë£Œ: descriptions_with_lexicon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b455f0-cf8a-441d-b895-1716716f4bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\SSAFY\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SSAFY\\.cache\\huggingface\\hub\\models--sentence-transformers--distiluse-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            description  lex_score  PN_final\n",
      "0         â„ ì—ì–´ì»¨ì„ 26ë„ë¡œ ì„¤ì •í•˜ê³  í•„ìš”í•œ ì‹œê°„ì—ë§Œ ì¼ ë‹¤.          0         1\n",
      "1             ğŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.          0         1\n",
      "2          ğŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.          0         1\n",
      "3              ğŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.          0         1\n",
      "4            ğŸ”Œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì „ìì œí’ˆì˜ í”ŒëŸ¬ê·¸ë¥¼ ë½‘ì•„ë‘”ë‹¤.          0        -1\n",
      "5          ğŸ¥˜ ìŒì‹ë¬¼ ì°Œêº¼ê¸°ë¥¼ ë¬¼ë¡œ ì”»ì–´ í•˜ìˆ˜êµ¬ì— í˜ë ¤ë³´ë‚¸ë‹¤.          0        -1\n",
      "6          ğŸ’§ ì„¸ì œë¥¼ ì‚¬ìš©í•œ ê±¸ë ˆë¥¼ íë¥´ëŠ” ë¬¼ë¡œ ì˜¤ë˜ í—¹êµ°ë‹¤.          0         1\n",
      "7       ğŸ’§ ê¸°ë¦„ ë¬»ì€ í›„ë¼ì´íŒ¬ì„ í‚¤ì¹œíƒ€ì›”ë¡œ ë‹¦ì€ í›„ ì„¤ê±°ì§€í•œë‹¤.          0         1\n",
      "8                 ğŸ—‘ í™”ì¥ì‹¤ì— ë¬¼í‹°ìŠˆë‚˜ ê¸°ì €ê·€ë¥¼ ë²„ë¦°ë‹¤.          0        -1\n",
      "9      ğŸ’§ ë‚¨ì€ ì•½ì´ë‚˜ íê¸°ë¬¼ì„ ì•½êµ­ ë˜ëŠ” ì§€ì • ìˆ˜ê±°í•¨ì— ë²„ë¦°ë‹¤.         -1        -1\n",
      "10               ğŸ›¢ ìš”ë¦¬ ì¤‘ ëƒ„ë¹„ ëšœê»‘ì„ ë®ê³  ì¡°ë¦¬í•œë‹¤.          0        -1\n",
      "11          ğŸ”¥ ê²¨ìš¸ì²  ë³´ì¼ëŸ¬ë¥¼ í•˜ë£¨ ì¢…ì¼ í‹€ì–´ë†“ê³  ì™¸ì¶œí•œë‹¤.          0        -1\n",
      "12     ğŸ”¥ ë³´ì¼ëŸ¬ ì˜¨ë„ëŠ” ë‚®ì¶”ê³ , ì˜¨ìˆ˜ëŠ” í•„ìš”í•œ ë§Œí¼ë§Œ ì‚¬ìš©í•œë‹¤.          0         1\n",
      "13         ğŸ›¢ ê°€ìŠ¤ë ˆì¸ì§€ì— ë¶ˆê½ƒì´ ëƒ„ë¹„ ë°”ê¹¥ìœ¼ë¡œ í¼ì§€ê²Œ ì¼ ë‹¤.          0        -1\n",
      "14  ğŸ›¢ ê°€ìŠ¤ ëˆ„ì¶œ ì ê²€ì„ ì •ê¸°ì ìœ¼ë¡œ í•˜ê³ , ì˜¤ë˜ëœ í˜¸ìŠ¤ë¥¼ êµì²´í•œë‹¤.          0         1\n",
      "15         ğŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.          0         1\n",
      "16            ğŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.          0         1\n",
      "17                        ğŸ§º ë§¤ì¼ ì„¸íƒê¸°ë¥¼ ëŒë¦°ë‹¤          0         1\n",
      "18             ğŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.          0         1\n",
      "19                      ğŸ’¡ ë¶ˆí•„ìš”í•œ ì¡°ëª…ì„ êº¼ë‘”ë‹¤.         -1        -1\n",
      "ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: descriptions_with_PN_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ê¸ì • ë‹¨ì–´ 100ê°œ (ê° ë‹¨ì–´ëŠ” ìµœëŒ€ 3ê¸€ì)\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ì¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ì¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ì¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ì\", \"ì •ì±…\",\n",
    "    \"ìì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ì¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìí™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìê²½\", \"ì¹œì\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìì§€\", \"ì—ê²½\", \"ì—ì •\", \"ì¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìí•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "# ë¶€ì • ë‹¨ì–´ 100ê°œ (ê° ë‹¨ì–´ëŠ” ìµœëŒ€ 3ê¸€ì)\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ì¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìí˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ì¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë‚´ ê¸ì •/ë¶€ì • ë‹¨ì–´ ê°œìˆ˜ë¥¼ ì„¸ì–´ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# ê° ë¬¸ì¥ì— ëŒ€í•´ ì‚¬ì „ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (lex_score)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words_100, negative_words_100))\n",
    "\n",
    "# sentence-transformers ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ ì‚¬ìš©)\n",
    "# í•œêµ­ì–´ì˜ ê²½ìš°, 'distiluse-base-multilingual-cased' ë˜ëŠ” 'paraphrase-multilingual-MiniLM-L12-v2'ì™€ ê°™ì´ ë‹¤êµ­ì–´ ëª¨ë¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ê¸ì • ë‹¨ì–´ ë° ë¶€ì • ë‹¨ì–´ì˜ ì„ë² ë”© ê³„ì‚° (ëª¨ë¸ì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ ëª¨ë‘ ì‚¬ìš©)\n",
    "pos_embs = [model.encode(word) for word in positive_words_100]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words_100]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"ë¬¸ì¥ì˜ ì„ë² ë”©ì„ êµ¬í•œ í›„, ê¸ì •/ë¶€ì • í‰ê·  ì„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    # ì‚¬ì „ ê¸°ë°˜ ì ìˆ˜ê°€ 0ì´ ì•„ë‹ˆë©´ ê·¸ ê²°ê³¼ ì‚¬ìš©, 0ì´ë©´ sentence-transformers ê¸°ë°˜ ìœ ì‚¬ë„ íŒë‹¨\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# ìµœì¢… ë¼ë²¨(PN_final) ë¶€ì—¬\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸ (ìƒìœ„ 20í–‰ ì¶œë ¥)\n",
    "print(df.head(20))\n",
    "\n",
    "# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "df.to_csv(\"./bulkdata/descriptions_with_PN_final.csv\", index=False)\n",
    "print(\"ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: descriptions_with_PN_final.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e934b452-9c2d-4f96-ac4f-1658206629c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "ì˜ˆì¸¡ ê²°ê³¼: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. CSV íŒŒì¼ì—ì„œ íŠ¸ë ˆì´ë‹ ë°ì´í„° ë¡œë“œ\n",
    "#    íŒŒì¼ì—ëŠ” 'description'ê³¼ 'PN' ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "df_train = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df_train.columns)\n",
    "\n",
    "# 2. ì‚¬ì „í•™ìŠµëœ SentenceTransformer ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì›)\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 3. í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±\n",
    "#    ê° ë¬¸ì¥ì„ ëª¨ë¸ì„ ì´ìš©í•´ ë²¡í„°í™”í•©ë‹ˆë‹¤.\n",
    "descriptions = df_train['description'].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df_train['PN'].values  # ë¼ë²¨: 1 (ê¸ì •), -1 (ë¶€ì •)\n",
    "\n",
    "# 4. ê°„ë‹¨í•œ ë¶„ë¥˜ê¸°(ë¡œì§€ìŠ¤í‹± íšŒê·€) í•™ìŠµ\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. ìƒˆë¡œìš´ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ ê¸ì •(1) ë˜ëŠ” ë¶€ì •(-1) ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): ì˜ˆì¸¡í•  ë¬¸ì¥\n",
    "        \n",
    "    Returns:\n",
    "        int: ì˜ˆì¸¡ëœ ë¼ë²¨ (1 ë˜ëŠ” -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# ì˜ˆì‹œ: í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ì˜ˆì¸¡\n",
    "test_sentence = \"ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "342ffab0-b5f1-4cf3-9b21-16ae9510f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\n",
      "ì „ì²´ ì •í™•ë„: 56.36%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.57      0.62        35\n",
      "           1       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.56      0.56      0.55        55\n",
      "weighted avg       0.59      0.56      0.57        55\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬:\n",
      "[[20 15]\n",
      " [ 9 11]]\n",
      "== ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\n",
      "ì „ì²´ ì •í™•ë„: 67.27%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      1.00      0.80        35\n",
      "           1       1.00      0.10      0.18        20\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.83      0.55      0.49        55\n",
      "weighted avg       0.78      0.67      0.57        55\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë¬¸ì¥: ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\n",
      "ì˜ˆì¸¡ ê²°ê³¼: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# â”€â”€ 1. ê¸°ë³¸ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ â”€â”€\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ì¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ì¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ì¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ì\", \"ì •ì±…\",\n",
    "    \"ìì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ì¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìí™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìê²½\", \"ì¹œì\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìì§€\", \"ì—ê²½\", \"ì—ì •\", \"ì¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìí•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ì¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìí˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ì¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "# â”€â”€ 2. ì¶”ê°€ ë‹¨ì–´ ì„¤ì • (ì‚¬ìš©ì ì •ì˜) â”€â”€\n",
    "additional_positive_words = [\"ì‹ ë¢°\", \"í’ˆì§ˆ\"]   # ì¶”ê°€ ê¸ì • ë‹¨ì–´\n",
    "additional_negative_words = [\"ë¶ˆë§Œ\", \"ë¬¸ì œ\"]     # ì¶”ê°€ ë¶€ì • ë‹¨ì–´\n",
    "\n",
    "# ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ì™€ ì¶”ê°€ ë‹¨ì–´ë¥¼ í•©ì¹¨\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë‚´ ê¸ì • ë‹¨ì–´ì™€ ë¶€ì • ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# â”€â”€ 3. CSV íŒŒì¼ ë¡œë“œ ë° ì •ë‹µ ë¼ë²¨(PN) í™œìš© â”€â”€\n",
    "# íŒŒì¼ì—ëŠ” ìµœì†Œí•œ 'description'ê³¼ ì •ë‹µ ë¼ë²¨ì¸ 'PN' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df.columns)\n",
    "\n",
    "# lexicon ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (ì¶”ê°€ ë‹¨ì–´ê¹Œì§€ ë°˜ì˜)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# â”€â”€ 4. SentenceTransformer ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© ê¸°ë°˜ ë¼ë²¨ ì‚°ì¶œ â”€â”€\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ê¸ì •/ë¶€ì • ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© í‰ê·  ê³„ì‚° (ëª¨ë“  ë‹¨ì–´ ì‚¬ìš©)\n",
    "pos_embs = [model.encode(word) for word in positive_words]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ì˜ ì„ë² ë”©ì„ êµ¬í•œ í›„, ê¸ì •/ë¶€ì • í‰ê·  ì„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    \"\"\"\n",
    "    lexicon ê¸°ë°˜ ì ìˆ˜ê°€ 0ì´ ì•„ë‹ˆë©´ í•´ë‹¹ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ê³ , 0ì´ë©´ ì„ë² ë”© ìœ ì‚¬ë„ ë¹„êµ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# lexicon ë° ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë¼ë²¨(PN_final) ì‚°ì¶œ\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# â”€â”€ 5. ì •ë‹µ(PN)ê³¼ ìš°ë¦¬ ë°©ë²•(PN_final) ë¹„êµ í‰ê°€ â”€â”€\n",
    "print(\"== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(df[\"PN\"], df[\"PN_final\"])*100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(df[\"PN\"], df[\"PN_final\"]))\n",
    "print(\"í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(df[\"PN\"], df[\"PN_final\"]))\n",
    "\n",
    "# â”€â”€ 6. ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ê¸° í•™ìŠµ (ë¬¸ì¥ ì„ë² ë”©ì„ ì´ìš©, ì •ë‹µ ë¼ë²¨ PN ì‚¬ìš©) â”€â”€\n",
    "descriptions = df[\"description\"].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df[\"PN\"].values  # ì •ë‹µ ë¼ë²¨ ì‚¬ìš©\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# â”€â”€ 7. ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„°ì—ì„œ ì˜ˆì‹œ) â”€â”€\n",
    "pred_train = clf.predict(X_train)\n",
    "print(\"== ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(y_train, pred_train)*100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_train, pred_train))\n",
    "\n",
    "# â”€â”€ 8. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ â”€â”€\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¥¼ ì´ìš©í•´ ê°ì„± ë¼ë²¨(1: ê¸ì •, -1: ë¶€ì •)ì„ ì˜ˆì¸¡í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): ì˜ˆì¸¡í•  ë¬¸ì¥\n",
    "        \n",
    "    Returns:\n",
    "        int: ì˜ˆì¸¡ëœ ê°ì„± ë¼ë²¨ (1 ë˜ëŠ” -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# â”€â”€ 9. í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ â”€â”€\n",
    "test_sentence = \"ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥:\", test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50f96365-1c1d-4a52-9cd4-5e47791dd531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\n",
      "ì „ì²´ ì •í™•ë„: 56.36%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.57      0.62        35\n",
      "           1       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.56      0.56      0.55        55\n",
      "weighted avg       0.59      0.56      0.57        55\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬:\n",
      "[[20 15]\n",
      " [ 9 11]]\n",
      "== SVM ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\n",
      "ì „ì²´ ì •í™•ë„: 67.27%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      1.00      0.80        35\n",
      "           1       1.00      0.10      0.18        20\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.83      0.55      0.49        55\n",
      "weighted avg       0.78      0.67      0.57        55\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë¬¸ì¥: ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\n",
      "ì˜ˆì¸¡ ê²°ê³¼: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# â”€â”€ 1. ê¸°ë³¸ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ â”€â”€\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ì¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ì¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ì¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ì\", \"ì •ì±…\",\n",
    "    \"ìì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ì¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìí™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìê²½\", \"ì¹œì\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìì§€\", \"ì—ê²½\", \"ì—ì •\", \"ì¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìí•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ì¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìí˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ì¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "# â”€â”€ 2. ì¶”ê°€ ë‹¨ì–´ ì„¤ì • (ì‚¬ìš©ì ì •ì˜) â”€â”€\n",
    "additional_positive_words = [\"ì‹ ë¢°\", \"í’ˆì§ˆ\"]   # ì¶”ê°€ ê¸ì • ë‹¨ì–´\n",
    "additional_negative_words = [\"ë¶ˆë§Œ\", \"ë¬¸ì œ\"]     # ì¶”ê°€ ë¶€ì • ë‹¨ì–´\n",
    "\n",
    "# ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ì™€ ì¶”ê°€ ë‹¨ì–´ë¥¼ í•©ì¹¨\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë‚´ ê¸ì • ë‹¨ì–´ì™€ ë¶€ì • ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# â”€â”€ 3. CSV íŒŒì¼ ë¡œë“œ ë° ì •ë‹µ ë¼ë²¨(PN) í™œìš© â”€â”€\n",
    "# íŒŒì¼ì—ëŠ” 'description'ê³¼ ì •ë‹µ ë¼ë²¨ 'PN' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df.columns)\n",
    "\n",
    "# lexicon ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (ì¶”ê°€ ë‹¨ì–´ê¹Œì§€ ë°˜ì˜)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# â”€â”€ 4. SentenceTransformer ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© ê¸°ë°˜ ë¼ë²¨ ì‚°ì¶œ â”€â”€\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ê¸ì •/ë¶€ì • ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© í‰ê·  ê³„ì‚° (ëª¨ë“  ë‹¨ì–´ ì‚¬ìš©)\n",
    "pos_embs = [model.encode(word) for word in positive_words]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ì˜ ì„ë² ë”©ì„ êµ¬í•œ í›„, ê¸ì •/ë¶€ì • í‰ê·  ì„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    \"\"\"\n",
    "    lexicon ê¸°ë°˜ ì ìˆ˜ê°€ 0ì´ ì•„ë‹ˆë©´ í•´ë‹¹ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ê³ , 0ì´ë©´ ì„ë² ë”© ìœ ì‚¬ë„ ë¹„êµ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# lexicon ë° ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë¼ë²¨(PN_final) ì‚°ì¶œ\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# â”€â”€ 5. ì •ë‹µ(PN)ê³¼ ìš°ë¦¬ ë°©ë²•(PN_final) ë¹„êµ í‰ê°€ â”€â”€\n",
    "print(\"== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(df[\"PN\"], df[\"PN_final\"]) * 100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(df[\"PN\"], df[\"PN_final\"]))\n",
    "print(\"í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(df[\"PN\"], df[\"PN_final\"]))\n",
    "\n",
    "# â”€â”€ 6. SVM ë¶„ë¥˜ê¸° í•™ìŠµ (ë¬¸ì¥ ì„ë² ë”©ì„ ì´ìš©, ì •ë‹µ ë¼ë²¨ PN ì‚¬ìš©) â”€â”€\n",
    "descriptions = df[\"description\"].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df[\"PN\"].values  # ì •ë‹µ ë¼ë²¨ ì‚¬ìš©\n",
    "\n",
    "# SVM ë¶„ë¥˜ê¸° (ì—¬ê¸°ì„œëŠ” Linear kernel ì‚¬ìš©)\n",
    "svm_clf = SVC(kernel='linear', probability=True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# â”€â”€ 7. ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„° í‰ê°€) â”€â”€\n",
    "pred_train = svm_clf.predict(X_train)\n",
    "print(\"== SVM ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(y_train, pred_train) * 100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_train, pred_train))\n",
    "\n",
    "# â”€â”€ 8. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ â”€â”€\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ í•™ìŠµëœ SVM ë¶„ë¥˜ê¸°ë¥¼ ì´ìš©í•´ ê°ì„± ë¼ë²¨(1: ê¸ì •, -1: ë¶€ì •)ì„ ì˜ˆì¸¡í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): ì˜ˆì¸¡í•  ë¬¸ì¥\n",
    "        \n",
    "    Returns:\n",
    "        int: ì˜ˆì¸¡ëœ ê°ì„± ë¼ë²¨ (1 ë˜ëŠ” -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = svm_clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# â”€â”€ 9. í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ â”€â”€\n",
    "test_sentence = \"ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥:\", test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d5ca12c-8886-4ec7-90f2-43a7c08d614f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatasetDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m num_epochs = \u001b[32m3\u001b[39m\n\u001b[32m     34\u001b[39m warmup_steps = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloader) * num_epochs * \u001b[32m0.1\u001b[39m)  \u001b[38;5;66;03m# ì•½ 10%ì˜ warmup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mmodel_ft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# fine tuning í›„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡\u001b[39;00m\n\u001b[32m     43\u001b[39m example_sentence = \u001b[33m\"\u001b[39m\u001b[33mì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\fit_mixin.py:290\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[39m\n\u001b[32m    287\u001b[39m         dataset = dataset.add_column(\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, labels)\n\u001b[32m    288\u001b[39m     train_dataset_dict[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_dataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = dataset\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m train_dataset_dict = \u001b[43mDatasetDict\u001b[49m(train_dataset_dict)\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_default_checkpoint_dir\u001b[39m() -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    293\u001b[39m     dir_name = \u001b[33m\"\u001b[39m\u001b[33mcheckpoints/model\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'DatasetDict' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import InputExample, SentenceTransformer, SentencesDataset, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import builtins\n",
    "builtins.Dataset = Dataset\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ (ì˜ˆ: 'description'ê³¼ ì •ë‹µ ë¼ë²¨ 'PN'ì´ í¬í•¨ëœ íŒŒì¼)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "\n",
    "# InputExample ê°ì²´ ìƒì„± (ì •ë‹µ ë¼ë²¨ì´ -1, 1ì¸ ê²½ìš°, SoftmaxLossë¥¼ ìœ„í•´ 0, 1ë¡œ ë³€í™˜)\n",
    "train_examples = []\n",
    "for idx, row in df.iterrows():\n",
    "    label = 0 if row['PN'] == -1 else 1\n",
    "    train_examples.append(InputExample(texts=[row['description']], label=label))\n",
    "\n",
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\n",
    "model_ft = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# SentencesDataset ë° DataLoader ì¤€ë¹„\n",
    "train_dataset = SentencesDataset(train_examples, model_ft)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "# ë¶„ë¥˜ë¥¼ ìœ„í•œ SoftmaxLoss ì„¤ì •\n",
    "num_labels = 2  # ë‘ í´ë˜ìŠ¤: 0 (ë¶€ì •), 1 (ê¸ì •)\n",
    "train_loss = losses.SoftmaxLoss(\n",
    "    model=model_ft, \n",
    "    sentence_embedding_dimension=model_ft.get_sentence_embedding_dimension(), \n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Fine tuning ì‹¤í–‰\n",
    "num_epochs = 3\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # ì•½ 10%ì˜ warmup\n",
    "model_ft.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# fine tuning í›„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡\n",
    "example_sentence = \"ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\"\n",
    "embedding = model_ft.encode(example_sentence)\n",
    "print(\"Fine tuned embedding:\", embedding[:5])  # ì„ë² ë”©ì˜ ì¼ë¶€ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ebc73-579a-4640-ac87-b2d5c5c93275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
