{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fcb4bf2-cd0b-4b25-8159-705e68d3a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 저장 완료\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./bulkdata/final_descriptions_with_scores.csv\")\n",
    "\n",
    "# 최종 DataFrame에서 description 컬럼만 선택\n",
    "final_description_df = df[['description']]\n",
    "\n",
    "# CSV 파일로 저장 (파일 경로는 필요에 따라 수정)\n",
    "final_description_df.to_csv(\"./bulkdata/descriptions.csv\", index=False)\n",
    "\n",
    "print(\"CSV 파일 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3cb5f6-dd0c-4773-925c-6fa28b6b5730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            description  PN\n",
      "0         ❄ 에어컨을 26도로 설정하고 필요한 시간에만 켠다.   1\n",
      "1             💡 집 안의 모든 조명을 형광등으로 유지한다.  -1\n",
      "2          🧺 세탁물을 모아서 세탁기를 주 2~3회만 돌린다.   1\n",
      "3              🔌 전기밥솥 보온 기능을 하루 종일 켜둔다.  -1\n",
      "4            🔌 사용하지 않는 전자제품의 플러그를 뽑아둔다.   1\n",
      "5          🥘 음식물 찌꺼기를 물로 씻어 하수구에 흘려보낸다.  -1\n",
      "6          💧 세제를 사용한 걸레를 흐르는 물로 오래 헹군다.  -1\n",
      "7       💧 기름 묻은 후라이팬을 키친타월로 닦은 후 설거지한다.   1\n",
      "8                 🗑 화장실에 물티슈나 기저귀를 버린다.  -1\n",
      "9      💧 남은 약이나 폐기물을 약국 또는 지정 수거함에 버린다.   1\n",
      "10               🛢 요리 중 냄비 뚜껑을 덮고 조리한다.   1\n",
      "11          🔥 겨울철 보일러를 하루 종일 틀어놓고 외출한다.  -1\n",
      "12     🔥 보일러 온도는 낮추고, 온수는 필요한 만큼만 사용한다.   1\n",
      "13         🛢 가스레인지에 불꽃이 냄비 바깥으로 퍼지게 켠다.   1\n",
      "14  🛢 가스 누출 점검을 정기적으로 하고, 오래된 호스를 교체한다.   1\n",
      "15         🧺 세탁물을 모아서 세탁기를 주 2~3회만 돌린다.   1\n",
      "16            💡 집 안의 모든 조명을 형광등으로 유지한다.  -1\n",
      "17                        🧺 매일 세탁기를 돌린다   1\n",
      "18             🔌 전기밥솥 보온 기능을 하루 종일 켜둔다.  -1\n",
      "19                      💡 불필요한 조명을 꺼둔다.  -1\n",
      "20                      ❄ 냉장고 문을 자주 연다.   1\n",
      "21                ❄ 에어컨 온도를 항상 낮게 유지한다.  -1\n",
      "22                       🔌 컴퓨터를 항상 켜둔다.  -1\n",
      "23                   🛢 요리 시 냄비 뚜껑을 덮는다.   1\n",
      "24              🔥 물을 끓일 때 항상 최대 불로 끓인다.   1\n",
      "25                 🛢 가스를 켜둔 채 다른 일을 한다.   1\n",
      "26                   🛢 매일 오랜 시간 난방을 한다.   1\n",
      "27                      💧 양치질 시 물을 잠근다.   1\n",
      "28                       💧 목욕탕을 매일 채운다.   1\n",
      "29                        💧 물을 많이 사용한다.   1\n",
      "30               💧 세차할 때 항상 물을 계속 틀어둔다.   1\n",
      "31           🔥 난방을 적절히 낮추고 옷을 따뜻하게 입는다.   1\n",
      "32                      ❄ 창문을 항상 열어 둔다.   1\n",
      "33                  🛢 집 전체를 하루 종일 난방한다.  -1\n",
      "34               🛢 난방기를 항상 최고 온도로 설정한다.   1\n",
      "35                      🗑 다회용 물병을 사용한다.   1\n",
      "36                    🗑 일회용 컵을 자주 사용한다.   1\n",
      "37              🗑 플라스틱 포장된 제품을 자주 구매한다.   1\n",
      "38                     🗑 비닐봉지를 자주 사용한다.   1\n",
      "39                       🗑 필요한 양만 조리한다.   1\n",
      "40                    🥘 항상 음식을 넉넉히 만든다.   1\n",
      "41                        🥘 음식을 자주 버린다.  -1\n",
      "42                  🥘 남은 음식 보관을 하지 않는다.   1\n",
      "43                   🚲 자전거 또는 걷기를 이용한다.   1\n",
      "44                        🚗 차를 자주 이용한다.   1\n",
      "45                  🚗 가까운 거리도 자동차로 다닌다.   1\n",
      "46                   🚗 대중교통 이용을 하지 않는다.   1\n",
      "47                         🗑 양면 인쇄를 한다.   1\n",
      "48                   🗑 종이를 일회용으로만 사용한다.   1\n",
      "49                       🗑 종이를 많이 사용한다.   1\n",
      "50                        🗑 한쪽 면만 인쇄한다.   1\n",
      "51          🌳 대기오염 감소와 에너지를 줄이는 효과가 있다.   1\n",
      "52                🚗 주차 공간 부족 문제를 심화시킨다.   1\n",
      "53      🚗 도로 확장을 유발하여 환경 파괴 원인이 될 수 있다.   1\n",
      "54       🛢 화석 연료 사용을 강제해 지구 온난화를 가속화한다.   1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "import io\n",
    "\n",
    "# 1. 학습 데이터 재정의 (사용자 레이블 및 피드백)\n",
    "user_labels_data = \"\"\"\n",
    "❄ 에어컨을 26도로 설정하고 필요한 시간에만 켠다.,1\n",
    "💡 집 안의 모든 조명을 형광등으로 유지한다.,-1\n",
    "🧺 세탁물을 모아서 세탁기를 주 2~3회만 돌린다.,1\n",
    "🔌 전기밥솥 보온 기능을 하루 종일 켜둔다.,-1\n",
    "🔌 사용하지 않는 전자제품의 플러그를 뽑아둔다.,1\n",
    "🥘 음식물 찌꺼기를 물로 씻어 하수구에 흘려보낸다.,-1\n",
    "💧 세제를 사용한 걸레를 흐르는 물로 오래 헹군다.,-1\n",
    "💧 기름 묻은 후라이팬을 키친타월로 닦은 후 설거지한다.,1\n",
    "🗑 화장실에 물티슈나 기저귀를 버린다.,-1\n",
    "💧 남은 약이나 폐기물을 약국 또는 지정 수거함에 버린다.,1\n",
    "🛢 요리 중 냄비 뚜껑을 덮고 조리한다.,1\n",
    "🔥 겨울철 보일러를 하루 종일 틀어놓고 외출한다.,-1\n",
    "\"\"\"\n",
    "\n",
    "# 데이터프레임 생성\n",
    "user_df = pd.read_csv(io.StringIO(user_labels_data), header=None, names=[\"description\", \"score\"])\n",
    "\n",
    "# 학습 데이터 결합 후, NaN 값 제거\n",
    "updated_train_data = updated_train_data.dropna(subset=[\"user_score\"])\n",
    "\n",
    "# 2. TF-IDF 벡터화 및 Ridge 회귀 모델 재학습\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(updated_train_data[\"description\"])\n",
    "y_train = updated_train_data[\"user_score\"].astype(float)\n",
    "\n",
    "reg_model = Ridge()\n",
    "reg_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. CSV 파일 불러오기 및 모델 예측\n",
    "df_reloaded = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# description 칼럼 기반 예측 수행\n",
    "X_all = vectorizer.transform(df_reloaded[\"description\"])\n",
    "reg_predictions = reg_model.predict(X_all)\n",
    "\n",
    "# 이진 긍/부정 판단: 0보다 크면 1(긍정), 그렇지 않으면 -1(부정)\n",
    "df_reloaded[\"PN\"] = [1 if score > 0 else -1 for score in reg_predictions]\n",
    "\n",
    "# 결과 미리 보기\n",
    "print(df_reloaded.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b07e429-2e27-463c-8b62-0fdd032bdc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "적용 완료: descriptions_with_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 긍정 단어 100개 (각 단어는 최대 3글자)\n",
    "positive_words_100 = [\n",
    "    \"절약\", \"효율\", \"친환\", \"재활\", \"보존\", \"절감\", \"최소\", \"지속\", \"절전\", \"저탄\",\n",
    "    \"개선\", \"정화\", \"순환\", \"재생\", \"녹색\", \"청정\", \"산림\", \"해양\", \"자연\", \"중립\",\n",
    "    \"다양\", \"에코\", \"혁신\", \"신재\", \"녹성\", \"회복\", \"도시\", \"경영\", \"투자\", \"정책\",\n",
    "    \"자원\", \"소비\", \"의식\", \"스마트\", \"녹기\", \"물절\", \"친농\", \"자립\", \"실천\", \"행동\",\n",
    "    \"지구\", \"환기\", \"환원\", \"감소\", \"조절\", \"에너\", \"산업\", \"녹산\", \"바람\", \"태양\",\n",
    "    \"수력\", \"풍력\", \"전환\", \"저배\", \"녹전\", \"친건\", \"친소\", \"친화\", \"녹업\", \"신기\",\n",
    "    \"산보\", \"해보\", \"친업\", \"전력\", \"녹율\", \"녹원\", \"친도\", \"녹지\", \"청원\", \"재순\",\n",
    "    \"친생\", \"환산\", \"탄감\", \"녹실\", \"녹환\", \"에녹\", \"신환\", \"자환\", \"청환\", \"녹창\",\n",
    "    \"에친\", \"효보\", \"에보\", \"산청\", \"녹청\", \"에신\", \"자경\", \"친자\", \"산지\", \"해지\",\n",
    "    \"자지\", \"에경\", \"에정\", \"재개\", \"녹개\", \"순보\", \"에순\", \"친합\", \"자합\", \"녹합\"\n",
    "]\n",
    "\n",
    "# 부정 단어 100개 (각 단어는 최대 3글자)\n",
    "negative_words_100 = [\n",
    "    \"낭비\", \"과다\", \"무분\", \"상시\", \"불필\", \"과도\", \"과소\", \"오염\", \"폐기\", \"파괴\",\n",
    "    \"배출\", \"독성\", \"부적\", \"비효\", \"무절\", \"자낭\", \"남용\", \"무용\", \"소음\", \"미세\",\n",
    "    \"화석\", \"비재\", \"독물\", \"온실\", \"과열\", \"불균\", \"불안\", \"폐물\", \"에낭\", \"무계\",\n",
    "    \"파적\", \"해로\", \"위협\", \"불안\", \"부실\", \"부정\", \"자형\", \"부실운\", \"오남\", \"무효\",\n",
    "    \"독배\", \"폐증\", \"소과\", \"불친\", \"비도\", \"위험\", \"낭성\", \"비합\", \"대오\", \"수오\",\n",
    "    \"토오\", \"소공\", \"오물\", \"해오\", \"기악\", \"불투\", \"산폐\", \"산파\", \"자파\", \"대질\",\n",
    "    \"자남\", \"화출\", \"유해\", \"폐수\", \"악취\", \"기유\", \"생파\", \"피해\", \"쓰범\", \"미폭\",\n",
    "    \"독폐\", \"불법\", \"불광\", \"비위\", \"무책\", \"폐부\", \"온과\", \"온난\", \"공해\", \"규미\",\n",
    "    \"피증\", \"재불\", \"무추\", \"쓰문\", \"도열\", \"온변\", \"위기\", \"생행\", \"불합\", \"비불\",\n",
    "    \"부퇴\", \"폐악\", \"불폐\", \"비낭\", \"비오\", \"과배\", \"부오\", \"비폐\", \"지오\", \"산오\"\n",
    "]\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"텍스트 내에 포함된 긍정 단어와 부정 단어의 개수를 세어 점수를 산출합니다.\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# CSV 파일 불러오기 (파일 경로를 실제 경로에 맞게 수정하세요)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# 각 문장에 대해 사전 기반 점수 계산 (lex_score)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words_100, negative_words_100))\n",
    "\n",
    "# 결과 CSV 파일로 저장 (예: descriptions_with_lexicon.csv)\n",
    "df.to_csv(\"./bulkdata/descriptions_with_lexicon.csv\", index=False)\n",
    "\n",
    "print(\"적용 완료: descriptions_with_lexicon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b455f0-cf8a-441d-b895-1716716f4bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\SSAFY\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SSAFY\\.cache\\huggingface\\hub\\models--sentence-transformers--distiluse-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            description  lex_score  PN_final\n",
      "0         ❄ 에어컨을 26도로 설정하고 필요한 시간에만 켠다.          0         1\n",
      "1             💡 집 안의 모든 조명을 형광등으로 유지한다.          0         1\n",
      "2          🧺 세탁물을 모아서 세탁기를 주 2~3회만 돌린다.          0         1\n",
      "3              🔌 전기밥솥 보온 기능을 하루 종일 켜둔다.          0         1\n",
      "4            🔌 사용하지 않는 전자제품의 플러그를 뽑아둔다.          0        -1\n",
      "5          🥘 음식물 찌꺼기를 물로 씻어 하수구에 흘려보낸다.          0        -1\n",
      "6          💧 세제를 사용한 걸레를 흐르는 물로 오래 헹군다.          0         1\n",
      "7       💧 기름 묻은 후라이팬을 키친타월로 닦은 후 설거지한다.          0         1\n",
      "8                 🗑 화장실에 물티슈나 기저귀를 버린다.          0        -1\n",
      "9      💧 남은 약이나 폐기물을 약국 또는 지정 수거함에 버린다.         -1        -1\n",
      "10               🛢 요리 중 냄비 뚜껑을 덮고 조리한다.          0        -1\n",
      "11          🔥 겨울철 보일러를 하루 종일 틀어놓고 외출한다.          0        -1\n",
      "12     🔥 보일러 온도는 낮추고, 온수는 필요한 만큼만 사용한다.          0         1\n",
      "13         🛢 가스레인지에 불꽃이 냄비 바깥으로 퍼지게 켠다.          0        -1\n",
      "14  🛢 가스 누출 점검을 정기적으로 하고, 오래된 호스를 교체한다.          0         1\n",
      "15         🧺 세탁물을 모아서 세탁기를 주 2~3회만 돌린다.          0         1\n",
      "16            💡 집 안의 모든 조명을 형광등으로 유지한다.          0         1\n",
      "17                        🧺 매일 세탁기를 돌린다          0         1\n",
      "18             🔌 전기밥솥 보온 기능을 하루 종일 켜둔다.          0         1\n",
      "19                      💡 불필요한 조명을 꺼둔다.         -1        -1\n",
      "최종 결과 저장 완료: descriptions_with_PN_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 긍정 단어 100개 (각 단어는 최대 3글자)\n",
    "positive_words_100 = [\n",
    "    \"절약\", \"효율\", \"친환\", \"재활\", \"보존\", \"절감\", \"최소\", \"지속\", \"절전\", \"저탄\",\n",
    "    \"개선\", \"정화\", \"순환\", \"재생\", \"녹색\", \"청정\", \"산림\", \"해양\", \"자연\", \"중립\",\n",
    "    \"다양\", \"에코\", \"혁신\", \"신재\", \"녹성\", \"회복\", \"도시\", \"경영\", \"투자\", \"정책\",\n",
    "    \"자원\", \"소비\", \"의식\", \"스마트\", \"녹기\", \"물절\", \"친농\", \"자립\", \"실천\", \"행동\",\n",
    "    \"지구\", \"환기\", \"환원\", \"감소\", \"조절\", \"에너\", \"산업\", \"녹산\", \"바람\", \"태양\",\n",
    "    \"수력\", \"풍력\", \"전환\", \"저배\", \"녹전\", \"친건\", \"친소\", \"친화\", \"녹업\", \"신기\",\n",
    "    \"산보\", \"해보\", \"친업\", \"전력\", \"녹율\", \"녹원\", \"친도\", \"녹지\", \"청원\", \"재순\",\n",
    "    \"친생\", \"환산\", \"탄감\", \"녹실\", \"녹환\", \"에녹\", \"신환\", \"자환\", \"청환\", \"녹창\",\n",
    "    \"에친\", \"효보\", \"에보\", \"산청\", \"녹청\", \"에신\", \"자경\", \"친자\", \"산지\", \"해지\",\n",
    "    \"자지\", \"에경\", \"에정\", \"재개\", \"녹개\", \"순보\", \"에순\", \"친합\", \"자합\", \"녹합\"\n",
    "]\n",
    "\n",
    "# 부정 단어 100개 (각 단어는 최대 3글자)\n",
    "negative_words_100 = [\n",
    "    \"낭비\", \"과다\", \"무분\", \"상시\", \"불필\", \"과도\", \"과소\", \"오염\", \"폐기\", \"파괴\",\n",
    "    \"배출\", \"독성\", \"부적\", \"비효\", \"무절\", \"자낭\", \"남용\", \"무용\", \"소음\", \"미세\",\n",
    "    \"화석\", \"비재\", \"독물\", \"온실\", \"과열\", \"불균\", \"불안\", \"폐물\", \"에낭\", \"무계\",\n",
    "    \"파적\", \"해로\", \"위협\", \"불안\", \"부실\", \"부정\", \"자형\", \"부실운\", \"오남\", \"무효\",\n",
    "    \"독배\", \"폐증\", \"소과\", \"불친\", \"비도\", \"위험\", \"낭성\", \"비합\", \"대오\", \"수오\",\n",
    "    \"토오\", \"소공\", \"오물\", \"해오\", \"기악\", \"불투\", \"산폐\", \"산파\", \"자파\", \"대질\",\n",
    "    \"자남\", \"화출\", \"유해\", \"폐수\", \"악취\", \"기유\", \"생파\", \"피해\", \"쓰범\", \"미폭\",\n",
    "    \"독폐\", \"불법\", \"불광\", \"비위\", \"무책\", \"폐부\", \"온과\", \"온난\", \"공해\", \"규미\",\n",
    "    \"피증\", \"재불\", \"무추\", \"쓰문\", \"도열\", \"온변\", \"위기\", \"생행\", \"불합\", \"비불\",\n",
    "    \"부퇴\", \"폐악\", \"불폐\", \"비낭\", \"비오\", \"과배\", \"부오\", \"비폐\", \"지오\", \"산오\"\n",
    "]\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"텍스트 내 긍정/부정 단어 개수를 세어 점수를 반환합니다.\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# CSV 파일 불러오기 (파일 경로를 실제 경로에 맞게 수정)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# 각 문장에 대해 사전 기반 점수 계산 (lex_score)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words_100, negative_words_100))\n",
    "\n",
    "# sentence-transformers 모델 로드 (다국어 지원 모델 사용)\n",
    "# 한국어의 경우, 'distiluse-base-multilingual-cased' 또는 'paraphrase-multilingual-MiniLM-L12-v2'와 같이 다국어 모델을 권장합니다.\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 긍정 단어 및 부정 단어의 임베딩 계산 (모델에 존재하는 단어 모두 사용)\n",
    "pos_embs = [model.encode(word) for word in positive_words_100]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words_100]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"문장의 임베딩을 구한 후, 긍정/부정 평균 임베딩과의 코사인 유사도를 비교하여 라벨을 반환합니다.\"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    # 사전 기반 점수가 0이 아니면 그 결과 사용, 0이면 sentence-transformers 기반 유사도 판단\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# 최종 라벨(PN_final) 부여\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# 결과 확인 (상위 20행 출력)\n",
    "print(df.head(20))\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "df.to_csv(\"./bulkdata/descriptions_with_PN_final.csv\", index=False)\n",
    "print(\"최종 결과 저장 완료: descriptions_with_PN_final.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e934b452-9c2d-4f96-ac4f-1658206629c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 컬럼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "예측 결과: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. CSV 파일에서 트레이닝 데이터 로드\n",
    "#    파일에는 'description'과 'PN' 컬럼이 있다고 가정합니다.\n",
    "df_train = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"데이터셋 컬럼:\", df_train.columns)\n",
    "\n",
    "# 2. 사전학습된 SentenceTransformer 모델 로드 (다국어 지원)\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 3. 학습 데이터에 대해 임베딩 생성\n",
    "#    각 문장을 모델을 이용해 벡터화합니다.\n",
    "descriptions = df_train['description'].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df_train['PN'].values  # 라벨: 1 (긍정), -1 (부정)\n",
    "\n",
    "# 4. 간단한 분류기(로지스틱 회귀) 학습\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. 새로운 문장을 입력받아 예측 결과를 반환하는 함수 정의\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    입력된 문장에 대해 긍정(1) 또는 부정(-1) 라벨을 반환합니다.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): 예측할 문장\n",
    "        \n",
    "    Returns:\n",
    "        int: 예측된 라벨 (1 또는 -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# 예시: 테스트 문장 예측\n",
    "test_sentence = \"이 제품은 정말 효율적이고 환경에도 좋아요.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"예측 결과:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "342ffab0-b5f1-4cf3-9b21-16ae9510f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 컬럼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "== Lexicon/Embedding 기반 예측 결과와 정답(PN) 비교 ==\n",
      "전체 정확도: 56.36%\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.57      0.62        35\n",
      "           1       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.56      0.56      0.55        55\n",
      "weighted avg       0.59      0.56      0.57        55\n",
      "\n",
      "혼동 행렬:\n",
      "[[20 15]\n",
      " [ 9 11]]\n",
      "== 분류기 학습 결과 (학습 데이터 평가) ==\n",
      "전체 정확도: 67.27%\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      1.00      0.80        35\n",
      "           1       1.00      0.10      0.18        20\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.83      0.55      0.49        55\n",
      "weighted avg       0.78      0.67      0.57        55\n",
      "\n",
      "테스트 문장: 이 제품은 정말 효율적이고 환경에도 좋아요.\n",
      "예측 결과: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ── 1. 기본 단어 리스트 ──\n",
    "positive_words_100 = [\n",
    "    \"절약\", \"효율\", \"친환\", \"재활\", \"보존\", \"절감\", \"최소\", \"지속\", \"절전\", \"저탄\",\n",
    "    \"개선\", \"정화\", \"순환\", \"재생\", \"녹색\", \"청정\", \"산림\", \"해양\", \"자연\", \"중립\",\n",
    "    \"다양\", \"에코\", \"혁신\", \"신재\", \"녹성\", \"회복\", \"도시\", \"경영\", \"투자\", \"정책\",\n",
    "    \"자원\", \"소비\", \"의식\", \"스마트\", \"녹기\", \"물절\", \"친농\", \"자립\", \"실천\", \"행동\",\n",
    "    \"지구\", \"환기\", \"환원\", \"감소\", \"조절\", \"에너\", \"산업\", \"녹산\", \"바람\", \"태양\",\n",
    "    \"수력\", \"풍력\", \"전환\", \"저배\", \"녹전\", \"친건\", \"친소\", \"친화\", \"녹업\", \"신기\",\n",
    "    \"산보\", \"해보\", \"친업\", \"전력\", \"녹율\", \"녹원\", \"친도\", \"녹지\", \"청원\", \"재순\",\n",
    "    \"친생\", \"환산\", \"탄감\", \"녹실\", \"녹환\", \"에녹\", \"신환\", \"자환\", \"청환\", \"녹창\",\n",
    "    \"에친\", \"효보\", \"에보\", \"산청\", \"녹청\", \"에신\", \"자경\", \"친자\", \"산지\", \"해지\",\n",
    "    \"자지\", \"에경\", \"에정\", \"재개\", \"녹개\", \"순보\", \"에순\", \"친합\", \"자합\", \"녹합\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"낭비\", \"과다\", \"무분\", \"상시\", \"불필\", \"과도\", \"과소\", \"오염\", \"폐기\", \"파괴\",\n",
    "    \"배출\", \"독성\", \"부적\", \"비효\", \"무절\", \"자낭\", \"남용\", \"무용\", \"소음\", \"미세\",\n",
    "    \"화석\", \"비재\", \"독물\", \"온실\", \"과열\", \"불균\", \"불안\", \"폐물\", \"에낭\", \"무계\",\n",
    "    \"파적\", \"해로\", \"위협\", \"불안\", \"부실\", \"부정\", \"자형\", \"부실운\", \"오남\", \"무효\",\n",
    "    \"독배\", \"폐증\", \"소과\", \"불친\", \"비도\", \"위험\", \"낭성\", \"비합\", \"대오\", \"수오\",\n",
    "    \"토오\", \"소공\", \"오물\", \"해오\", \"기악\", \"불투\", \"산폐\", \"산파\", \"자파\", \"대질\",\n",
    "    \"자남\", \"화출\", \"유해\", \"폐수\", \"악취\", \"기유\", \"생파\", \"피해\", \"쓰범\", \"미폭\",\n",
    "    \"독폐\", \"불법\", \"불광\", \"비위\", \"무책\", \"폐부\", \"온과\", \"온난\", \"공해\", \"규미\",\n",
    "    \"피증\", \"재불\", \"무추\", \"쓰문\", \"도열\", \"온변\", \"위기\", \"생행\", \"불합\", \"비불\",\n",
    "    \"부퇴\", \"폐악\", \"불폐\", \"비낭\", \"비오\", \"과배\", \"부오\", \"비폐\", \"지오\", \"산오\"\n",
    "]\n",
    "\n",
    "# ── 2. 추가 단어 설정 (사용자 정의) ──\n",
    "additional_positive_words = [\"신뢰\", \"품질\"]   # 추가 긍정 단어\n",
    "additional_negative_words = [\"불만\", \"문제\"]     # 추가 부정 단어\n",
    "\n",
    "# 기본 리스트와 추가 단어를 합침\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"\n",
    "    텍스트 내 긍정 단어와 부정 단어 등장 횟수를 비교하여 점수를 반환합니다.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# ── 3. CSV 파일 로드 및 정답 라벨(PN) 활용 ──\n",
    "# 파일에는 최소한 'description'과 정답 라벨인 'PN' 컬럼이 있어야 합니다.\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"데이터셋 컬럼:\", df.columns)\n",
    "\n",
    "# lexicon 기반 점수 계산 (추가 단어까지 반영)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# ── 4. SentenceTransformer 모델 로드 및 임베딩 기반 라벨 산출 ──\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 긍정/부정 단어들의 임베딩 평균 계산 (모든 단어 사용)\n",
    "pos_embs = [model.encode(word) for word in positive_words]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"\n",
    "    문장의 임베딩을 구한 후, 긍정/부정 평균 임베딩과의 코사인 유사도를 비교하여 라벨을 반환합니다.\n",
    "    \"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    \"\"\"\n",
    "    lexicon 기반 점수가 0이 아니면 해당 결과를 사용하고, 0이면 임베딩 유사도 비교 결과를 사용합니다.\n",
    "    \"\"\"\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# lexicon 및 임베딩 기반으로 최종 라벨(PN_final) 산출\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# ── 5. 정답(PN)과 우리 방법(PN_final) 비교 평가 ──\n",
    "print(\"== Lexicon/Embedding 기반 예측 결과와 정답(PN) 비교 ==\")\n",
    "print(\"전체 정확도: {:.2f}%\".format(accuracy_score(df[\"PN\"], df[\"PN_final\"])*100))\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(df[\"PN\"], df[\"PN_final\"]))\n",
    "print(\"혼동 행렬:\")\n",
    "print(confusion_matrix(df[\"PN\"], df[\"PN_final\"]))\n",
    "\n",
    "# ── 6. 로지스틱 회귀 분류기 학습 (문장 임베딩을 이용, 정답 라벨 PN 사용) ──\n",
    "descriptions = df[\"description\"].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df[\"PN\"].values  # 정답 라벨 사용\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ── 7. 분류기 평가 (학습 데이터에서 예시) ──\n",
    "pred_train = clf.predict(X_train)\n",
    "print(\"== 분류기 학습 결과 (학습 데이터 평가) ==\")\n",
    "print(\"전체 정확도: {:.2f}%\".format(accuracy_score(y_train, pred_train)*100))\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(y_train, pred_train))\n",
    "\n",
    "# ── 8. 예측 함수 정의 ──\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    입력된 문장에 대해 학습된 분류기를 이용해 감성 라벨(1: 긍정, -1: 부정)을 예측하여 반환합니다.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): 예측할 문장\n",
    "        \n",
    "    Returns:\n",
    "        int: 예측된 감성 라벨 (1 또는 -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# ── 9. 테스트 예시 ──\n",
    "test_sentence = \"이 제품은 정말 효율적이고 환경에도 좋아요.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"테스트 문장:\", test_sentence)\n",
    "print(\"예측 결과:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f96365-1c1d-4a52-9cd4-5e47791dd531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 컬럼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "== Lexicon/Embedding 기반 예측 결과와 정답(PN) 비교 ==\n",
      "전체 정확도: 56.36%\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.57      0.62        35\n",
      "           1       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.56      0.56      0.55        55\n",
      "weighted avg       0.59      0.56      0.57        55\n",
      "\n",
      "혼동 행렬:\n",
      "[[20 15]\n",
      " [ 9 11]]\n",
      "== SVM 분류기 학습 결과 (학습 데이터 평가) ==\n",
      "전체 정확도: 67.27%\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      1.00      0.80        35\n",
      "           1       1.00      0.10      0.18        20\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.83      0.55      0.49        55\n",
      "weighted avg       0.78      0.67      0.57        55\n",
      "\n",
      "테스트 문장: 대중교통을 타지 않는다\n",
      "예측 결과: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ── 1. 기본 단어 리스트 ──\n",
    "positive_words_100 = [\n",
    "    \"절약\", \"효율\", \"친환\", \"재활\", \"보존\", \"절감\", \"최소\", \"지속\", \"절전\", \"저탄\",\n",
    "    \"개선\", \"정화\", \"순환\", \"재생\", \"녹색\", \"청정\", \"산림\", \"해양\", \"자연\", \"중립\",\n",
    "    \"다양\", \"에코\", \"혁신\", \"신재\", \"녹성\", \"회복\", \"도시\", \"경영\", \"투자\", \"정책\",\n",
    "    \"자원\", \"소비\", \"의식\", \"스마트\", \"녹기\", \"물절\", \"친농\", \"자립\", \"실천\", \"행동\",\n",
    "    \"지구\", \"환기\", \"환원\", \"감소\", \"조절\", \"에너\", \"산업\", \"녹산\", \"바람\", \"태양\",\n",
    "    \"수력\", \"풍력\", \"전환\", \"저배\", \"녹전\", \"친건\", \"친소\", \"친화\", \"녹업\", \"신기\",\n",
    "    \"산보\", \"해보\", \"친업\", \"전력\", \"녹율\", \"녹원\", \"친도\", \"녹지\", \"청원\", \"재순\",\n",
    "    \"친생\", \"환산\", \"탄감\", \"녹실\", \"녹환\", \"에녹\", \"신환\", \"자환\", \"청환\", \"녹창\",\n",
    "    \"에친\", \"효보\", \"에보\", \"산청\", \"녹청\", \"에신\", \"자경\", \"친자\", \"산지\", \"해지\",\n",
    "    \"자지\", \"에경\", \"에정\", \"재개\", \"녹개\", \"순보\", \"에순\", \"친합\", \"자합\", \"녹합\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"낭비\", \"과다\", \"무분\", \"상시\", \"불필\", \"과도\", \"과소\", \"오염\", \"폐기\", \"파괴\",\n",
    "    \"배출\", \"독성\", \"부적\", \"비효\", \"무절\", \"자낭\", \"남용\", \"무용\", \"소음\", \"미세\",\n",
    "    \"화석\", \"비재\", \"독물\", \"온실\", \"과열\", \"불균\", \"불안\", \"폐물\", \"에낭\", \"무계\",\n",
    "    \"파적\", \"해로\", \"위협\", \"불안\", \"부실\", \"부정\", \"자형\", \"부실운\", \"오남\", \"무효\",\n",
    "    \"독배\", \"폐증\", \"소과\", \"불친\", \"비도\", \"위험\", \"낭성\", \"비합\", \"대오\", \"수오\",\n",
    "    \"토오\", \"소공\", \"오물\", \"해오\", \"기악\", \"불투\", \"산폐\", \"산파\", \"자파\", \"대질\",\n",
    "    \"자남\", \"화출\", \"유해\", \"폐수\", \"악취\", \"기유\", \"생파\", \"피해\", \"쓰범\", \"미폭\",\n",
    "    \"독폐\", \"불법\", \"불광\", \"비위\", \"무책\", \"폐부\", \"온과\", \"온난\", \"공해\", \"규미\",\n",
    "    \"피증\", \"재불\", \"무추\", \"쓰문\", \"도열\", \"온변\", \"위기\", \"생행\", \"불합\", \"비불\",\n",
    "    \"부퇴\", \"폐악\", \"불폐\", \"비낭\", \"비오\", \"과배\", \"부오\", \"비폐\", \"지오\", \"산오\"\n",
    "]\n",
    "\n",
    "# ── 2. 추가 단어 설정 (사용자 정의) ──\n",
    "additional_positive_words = [\"신뢰\", \"품질\"]   # 추가 긍정 단어\n",
    "additional_negative_words = [\"불만\", \"문제\"]     # 추가 부정 단어\n",
    "\n",
    "# 기본 리스트와 추가 단어를 합침\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"\n",
    "    텍스트 내 긍정 단어와 부정 단어 등장 횟수를 비교하여 점수를 반환합니다.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# ── 3. CSV 파일 로드 및 정답 라벨(PN) 활용 ──\n",
    "# 파일에는 'description'과 정답 라벨 'PN' 컬럼이 있어야 합니다.\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"데이터셋 컬럼:\", df.columns)\n",
    "\n",
    "# lexicon 기반 점수 계산 (추가 단어까지 반영)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# ── 4. SentenceTransformer 모델 로드 및 임베딩 기반 라벨 산출 ──\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 긍정/부정 단어들의 임베딩 평균 계산 (모든 단어 사용)\n",
    "pos_embs = [model.encode(word) for word in positive_words]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"\n",
    "    문장의 임베딩을 구한 후, 긍정/부정 평균 임베딩과의 코사인 유사도를 비교하여 라벨을 반환합니다.\n",
    "    \"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    \"\"\"\n",
    "    lexicon 기반 점수가 0이 아니면 해당 결과를 사용하고, 0이면 임베딩 유사도 비교 결과를 사용합니다.\n",
    "    \"\"\"\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# lexicon 및 임베딩 기반으로 최종 라벨(PN_final) 산출\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# ── 5. 정답(PN)과 우리 방법(PN_final) 비교 평가 ──\n",
    "print(\"== Lexicon/Embedding 기반 예측 결과와 정답(PN) 비교 ==\")\n",
    "print(\"전체 정확도: {:.2f}%\".format(accuracy_score(df[\"PN\"], df[\"PN_final\"]) * 100))\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(df[\"PN\"], df[\"PN_final\"]))\n",
    "print(\"혼동 행렬:\")\n",
    "print(confusion_matrix(df[\"PN\"], df[\"PN_final\"]))\n",
    "\n",
    "# ── 6. SVM 분류기 학습 (문장 임베딩을 이용, 정답 라벨 PN 사용) ──\n",
    "descriptions = df[\"description\"].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df[\"PN\"].values  # 정답 라벨 사용\n",
    "\n",
    "# SVM 분류기 (여기서는 Linear kernel 사용)\n",
    "svm_clf = SVC(kernel='linear', probability=True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# ── 7. 분류기 평가 (학습 데이터 평가) ──\n",
    "pred_train = svm_clf.predict(X_train)\n",
    "print(\"== SVM 분류기 학습 결과 (학습 데이터 평가) ==\")\n",
    "print(\"전체 정확도: {:.2f}%\".format(accuracy_score(y_train, pred_train) * 100))\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(y_train, pred_train))\n",
    "\n",
    "# ── 8. 예측 함수 정의 ──\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    입력된 문장에 대해 학습된 SVM 분류기를 이용해 감성 라벨(1: 긍정, -1: 부정)을 예측하여 반환합니다.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): 예측할 문장\n",
    "        \n",
    "    Returns:\n",
    "        int: 예측된 감성 라벨 (1 또는 -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = svm_clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# ── 9. 테스트 예시 ──\n",
    "test_sentence = \"대중교통을 타지 않는다\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"테스트 문장:\", test_sentence)\n",
    "print(\"예측 결과:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5ca12c-8886-4ec7-90f2-43a7c08d614f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (25) must match the size of tensor b (16) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m num_epochs = \u001b[32m3\u001b[39m\n\u001b[32m     58\u001b[39m warmup_steps = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloader) * num_epochs * \u001b[32m0.1\u001b[39m)  \u001b[38;5;66;03m# 약 10%의 warmup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# fine tuning 후 모델을 사용하여 예측 (분류 logits 생성)\u001b[39;00m\n\u001b[32m     67\u001b[39m example_sentence = \u001b[33m\"\u001b[39m\u001b[33m이 제품은 정말 효율적이고 환경에도 좋아요.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\fit_mixin.py:408\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[39m\n\u001b[32m    405\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint directory does not exist or is not a directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    406\u001b[39m         resume_from_checkpoint = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\trainer.py:2556\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2549\u001b[39m context = (\n\u001b[32m   2550\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2553\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2554\u001b[39m )\n\u001b[32m   2555\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2556\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2559\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2561\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2562\u001b[39m ):\n\u001b[32m   2563\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2564\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\trainer.py:3718\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3715\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3717\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3718\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3720\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3722\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3723\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3724\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\trainer.py:407\u001b[39m, in \u001b[36mSentenceTransformerTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    401\u001b[39m     model == \u001b[38;5;28mself\u001b[39m.model_wrapped\n\u001b[32m    402\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m model != \u001b[38;5;28mself\u001b[39m.model  \u001b[38;5;66;03m# Only if the model is wrapped\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(loss_fn, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Only if the loss stores the model\u001b[39;00m\n\u001b[32m    404\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m loss_fn.model != model  \u001b[38;5;66;03m# Only if the wrapped model is not already stored\u001b[39;00m\n\u001b[32m    405\u001b[39m ):\n\u001b[32m    406\u001b[39m     loss_fn = \u001b[38;5;28mself\u001b[39m.override_model_in_loss(loss_fn, model)\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n\u001b[32m    409\u001b[39m     \u001b[38;5;66;03m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[39;00m\n\u001b[32m    410\u001b[39m     \u001b[38;5;66;03m# However, Sentence Transformer losses do not return outputs, so we return an empty dictionary.\u001b[39;00m\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# This does not result in any problems, as the SentenceTransformerTrainingArguments sets\u001b[39;00m\n\u001b[32m    412\u001b[39m     \u001b[38;5;66;03m# `prediction_loss_only=True` which means that the output is not used.\u001b[39;00m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mClassificationLoss.forward\u001b[39m\u001b[34m(self, sentence_features, labels)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     49\u001b[39m     batched_features = sentence_features\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss_fct(logits, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\SentenceTransformer.py:619\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor], **kwargs) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module_name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.named_children():\n\u001b[32m    622\u001b[39m         module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m output_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m output_tokens = output_states[\u001b[32m0\u001b[39m]\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:784\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    782\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_flash_attention_2:\n\u001b[32m    787\u001b[39m     attention_mask = attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:131\u001b[39m, in \u001b[36mEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, input_embeds)\u001b[39m\n\u001b[32m    127\u001b[39m     position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m).expand_as(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length)\u001b[39;00m\n\u001b[32m    129\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.position_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m embeddings = \u001b[43minput_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m    132\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.LayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m    133\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.dropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (25) must match the size of tensor b (16) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import InputExample, SentenceTransformer, SentencesDataset\n",
    "from sentence_transformers.models import Transformer, Pooling, Dense\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# CSV 파일 로드 (예: 'description'과 정답 라벨 'PN'이 포함된 파일)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "\n",
    "# InputExample 객체 생성 (정답 라벨이 -1, 1인 경우, 분류를 위해 0, 1로 변환)\n",
    "train_examples = []\n",
    "for idx, row in df.iterrows():\n",
    "    label = 0 if row['PN'] == -1 else 1\n",
    "    train_examples.append(InputExample(texts=[row['description']], label=label))\n",
    "\n",
    "num_labels = 2  # 부정(0), 긍정(1)\n",
    "\n",
    "# 모델 구성: Transformer -> Pooling -> Dense(분류 헤드)\n",
    "model_name = 'sentence-transformers/distiluse-base-multilingual-cased'\n",
    "word_embedding_model = Transformer(model_name, max_seq_length=128)\n",
    "pooling_model = Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "classification_head = Dense(\n",
    "    in_features=pooling_model.get_sentence_embedding_dimension(),\n",
    "    out_features=num_labels,\n",
    "    activation_function=nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, classification_head])\n",
    "\n",
    "# 학습 데이터셋 및 DataLoader 생성\n",
    "train_dataset = SentencesDataset(train_examples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "# 커스텀 분류 손실 함수 (배치 내 리스트를 딕셔너리로 변환)\n",
    "class ClassificationLoss(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ClassificationLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, sentence_features, labels):\n",
    "        # 배치가 리스트인 경우 딕셔너리 형태로 변환\n",
    "        if isinstance(sentence_features, list):\n",
    "            batched_features = {}\n",
    "            for key in sentence_features[0].keys():\n",
    "                batched_features[key] = torch.stack([sf[key] for sf in sentence_features], dim=0)\n",
    "        else:\n",
    "            batched_features = sentence_features\n",
    "\n",
    "        logits = self.model(batched_features)\n",
    "        return self.loss_fct(logits, labels)\n",
    "\n",
    "train_loss = ClassificationLoss(model)\n",
    "\n",
    "# Fine tuning 실행\n",
    "num_epochs = 3\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # 약 10%의 warmup\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# fine tuning 후 모델을 사용하여 예측 (분류 logits 생성)\n",
    "example_sentence = \"이 제품은 정말 효율적이고 환경에도 좋아요.\"\n",
    "inputs = model.tokenizer(example_sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(inputs)\n",
    "print(\"Classification logits:\", logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3ebc73-579a-4640-ac87-b2d5c5c93275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(\"Accelerate version:\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88e2a37-13cb-4636-9cde-7b53c9ed817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word_embedding_model.auto_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322b1f0a-7e4e-481d-ae03-bf5816e439ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 컬럼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "== Lexicon/Embedding 기반 예측 결과와 정답(PN) 비교 ==\n",
      "전체 정확도: 56.36%\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.57      0.62        35\n",
      "           1       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.56      0.56      0.55        55\n",
      "weighted avg       0.59      0.56      0.57        55\n",
      "\n",
      "혼동 행렬:\n",
      "[[20 15]\n",
      " [ 9 11]]\n",
      "== 분류기 학습 결과 (학습 데이터 평가) ==\n",
      "전체 정확도: 67.27%\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      1.00      0.80        35\n",
      "           1       1.00      0.10      0.18        20\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.83      0.55      0.49        55\n",
      "weighted avg       0.78      0.67      0.57        55\n",
      "\n",
      "테스트 문장: 이 제품은 정말 효율적이고 환경에도 좋아요.\n",
      "예측 결과: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 구두점 제거\n",
    "    return text\n",
    "\n",
    "# ── 1. 기본 단어 리스트 ──\n",
    "positive_words_100 = [\n",
    "    \"절약\", \"효율\", \"친환\", \"재활\", \"보존\", \"절감\", \"최소\", \"지속\", \"절전\", \"저탄\",\n",
    "    \"개선\", \"정화\", \"순환\", \"재생\", \"녹색\", \"청정\", \"산림\", \"해양\", \"자연\", \"중립\",\n",
    "    \"다양\", \"에코\", \"혁신\", \"신재\", \"녹성\", \"회복\", \"도시\", \"경영\", \"투자\", \"정책\",\n",
    "    \"자원\", \"소비\", \"의식\", \"스마트\", \"녹기\", \"물절\", \"친농\", \"자립\", \"실천\", \"행동\",\n",
    "    \"지구\", \"환기\", \"환원\", \"감소\", \"조절\", \"에너\", \"산업\", \"녹산\", \"바람\", \"태양\",\n",
    "    \"수력\", \"풍력\", \"전환\", \"저배\", \"녹전\", \"친건\", \"친소\", \"친화\", \"녹업\", \"신기\",\n",
    "    \"산보\", \"해보\", \"친업\", \"전력\", \"녹율\", \"녹원\", \"친도\", \"녹지\", \"청원\", \"재순\",\n",
    "    \"친생\", \"환산\", \"탄감\", \"녹실\", \"녹환\", \"에녹\", \"신환\", \"자환\", \"청환\", \"녹창\",\n",
    "    \"에친\", \"효보\", \"에보\", \"산청\", \"녹청\", \"에신\", \"자경\", \"친자\", \"산지\", \"해지\",\n",
    "    \"자지\", \"에경\", \"에정\", \"재개\", \"녹개\", \"순보\", \"에순\", \"친합\", \"자합\", \"녹합\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"낭비\", \"과다\", \"무분\", \"상시\", \"불필\", \"과도\", \"과소\", \"오염\", \"폐기\", \"파괴\",\n",
    "    \"배출\", \"독성\", \"부적\", \"비효\", \"무절\", \"자낭\", \"남용\", \"무용\", \"소음\", \"미세\",\n",
    "    \"화석\", \"비재\", \"독물\", \"온실\", \"과열\", \"불균\", \"불안\", \"폐물\", \"에낭\", \"무계\",\n",
    "    \"파적\", \"해로\", \"위협\", \"불안\", \"부실\", \"부정\", \"자형\", \"부실운\", \"오남\", \"무효\",\n",
    "    \"독배\", \"폐증\", \"소과\", \"불친\", \"비도\", \"위험\", \"낭성\", \"비합\", \"대오\", \"수오\",\n",
    "    \"토오\", \"소공\", \"오물\", \"해오\", \"기악\", \"불투\", \"산폐\", \"산파\", \"자파\", \"대질\",\n",
    "    \"자남\", \"화출\", \"유해\", \"폐수\", \"악취\", \"기유\", \"생파\", \"피해\", \"쓰범\", \"미폭\",\n",
    "    \"독폐\", \"불법\", \"불광\", \"비위\", \"무책\", \"폐부\", \"온과\", \"온난\", \"공해\", \"규미\",\n",
    "    \"피증\", \"재불\", \"무추\", \"쓰문\", \"도열\", \"온변\", \"위기\", \"생행\", \"불합\", \"비불\",\n",
    "    \"부퇴\", \"폐악\", \"불폐\", \"비낭\", \"비오\", \"과배\", \"부오\", \"비폐\", \"지오\", \"산오\"\n",
    "]\n",
    "\n",
    "# ── 2. 추가 단어 설정 (사용자 정의) ──\n",
    "additional_positive_words = [\"신뢰\", \"품질\"]   # 추가 긍정 단어\n",
    "additional_negative_words = [\"불만\", \"문제\"]     # 추가 부정 단어\n",
    "\n",
    "# 기본 리스트와 추가 단어를 합침\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"\n",
    "    텍스트 내 긍정 단어와 부정 단어 등장 횟수를 비교하여 점수를 반환합니다.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# ── 3. CSV 파일 로드 및 정답 라벨(PN) 활용 ──\n",
    "# 파일에는 최소한 'description'과 정답 라벨인 'PN' 컬럼이 있어야 합니다.\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"데이터셋 컬럼:\", df.columns)\n",
    "\n",
    "# lexicon 기반 점수 계산 (추가 단어까지 반영)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(clean_text).apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# ── 4. SentenceTransformer 모델 로드 및 임베딩 기반 라벨 산출 ──\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 긍정/부정 단어들의 임베딩 평균 계산 (모든 단어 사용)\n",
    "pos_embs = [model.encode(word) for word in positive_words]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"\n",
    "    문장의 임베딩을 구한 후, 긍정/부정 평균 임베딩과의 코사인 유사도를 비교하여 라벨을 반환합니다.\n",
    "    \"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    \"\"\"\n",
    "    lexicon 기반 점수가 0이 아니면 해당 결과를 사용하고, 0이면 임베딩 유사도 비교 결과를 사용합니다.\n",
    "    \"\"\"\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# lexicon 및 임베딩 기반으로 최종 라벨(PN_final) 산출\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# ── 5. 정답(PN)과 우리 방법(PN_final) 비교 평가 ──\n",
    "print(\"== Lexicon/Embedding 기반 예측 결과와 정답(PN) 비교 ==\")\n",
    "print(\"전체 정확도: {:.2f}%\".format(accuracy_score(df[\"PN\"], df[\"PN_final\"])*100))\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(df[\"PN\"], df[\"PN_final\"]))\n",
    "print(\"혼동 행렬:\")\n",
    "print(confusion_matrix(df[\"PN\"], df[\"PN_final\"]))\n",
    "\n",
    "# ── 6. 로지스틱 회귀 분류기 학습 (문장 임베딩을 이용, 정답 라벨 PN 사용) ──\n",
    "descriptions = df[\"description\"].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df[\"PN\"].values  # 정답 라벨 사용\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ── 7. 분류기 평가 (학습 데이터에서 예시) ──\n",
    "pred_train = clf.predict(X_train)\n",
    "print(\"== 분류기 학습 결과 (학습 데이터 평가) ==\")\n",
    "print(\"전체 정확도: {:.2f}%\".format(accuracy_score(y_train, pred_train)*100))\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(y_train, pred_train))\n",
    "\n",
    "# ── 8. 예측 함수 정의 ──\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    입력된 문장에 대해 학습된 분류기를 이용해 감성 라벨(1: 긍정, -1: 부정)을 예측하여 반환합니다.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): 예측할 문장\n",
    "        \n",
    "    Returns:\n",
    "        int: 예측된 감성 라벨 (1 또는 -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# ── 9. 테스트 예시 ──\n",
    "test_sentence = \"이 제품은 정말 효율적이고 환경에도 좋아요.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"테스트 문장:\", test_sentence)\n",
    "print(\"예측 결과:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab31b0f-5ebe-46d1-b0f3-b4bd2248ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 컬럼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "최적의 파라미터: {'C': 10}\n",
      "그리드 서치 최고 정확도: 72.73%\n",
      "== SVM 분류기 평가 (학습 데이터) ==\n",
      "전체 정확도: 96.36%\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      1.00      0.97        35\n",
      "           1       1.00      0.90      0.95        20\n",
      "\n",
      "    accuracy                           0.96        55\n",
      "   macro avg       0.97      0.95      0.96        55\n",
      "weighted avg       0.97      0.96      0.96        55\n",
      "\n",
      "혼동 행렬:\n",
      "[[35  0]\n",
      " [ 2 18]]\n",
      "테스트 문장: 대중교통을 탄다\n",
      "예측 결과: -1\n",
      "적용 완료: descriptions_with_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── 1. 단어 리스트 (기본 + 추가) ──\n",
    "positive_words_100 = [\n",
    "    \"절약\", \"효율\", \"친환\", \"재활\", \"보존\", \"절감\", \"최소\", \"지속\", \"절전\", \"저탄\",\n",
    "    \"개선\", \"정화\", \"순환\", \"재생\", \"녹색\", \"청정\", \"산림\", \"해양\", \"자연\", \"중립\",\n",
    "    \"다양\", \"에코\", \"혁신\", \"신재\", \"녹성\", \"회복\", \"도시\", \"경영\", \"투자\", \"정책\",\n",
    "    \"자원\", \"소비\", \"의식\", \"스마트\", \"녹기\", \"물절\", \"친농\", \"자립\", \"실천\", \"행동\",\n",
    "    \"지구\", \"환기\", \"환원\", \"감소\", \"조절\", \"에너\", \"산업\", \"녹산\", \"바람\", \"태양\",\n",
    "    \"수력\", \"풍력\", \"전환\", \"저배\", \"녹전\", \"친건\", \"친소\", \"친화\", \"녹업\", \"신기\",\n",
    "    \"산보\", \"해보\", \"친업\", \"전력\", \"녹율\", \"녹원\", \"친도\", \"녹지\", \"청원\", \"재순\",\n",
    "    \"친생\", \"환산\", \"탄감\", \"녹실\", \"녹환\", \"에녹\", \"신환\", \"자환\", \"청환\", \"녹창\",\n",
    "    \"에친\", \"효보\", \"에보\", \"산청\", \"녹청\", \"에신\", \"자경\", \"친자\", \"산지\", \"해지\",\n",
    "    \"자지\", \"에경\", \"에정\", \"재개\", \"녹개\", \"순보\", \"에순\", \"친합\", \"자합\", \"녹합\"\n",
    "]\n",
    "negative_words_100 = [\n",
    "    \"낭비\", \"과다\", \"무분\", \"상시\", \"불필\", \"과도\", \"과소\", \"오염\", \"폐기\", \"파괴\",\n",
    "    \"배출\", \"독성\", \"부적\", \"비효\", \"무절\", \"자낭\", \"남용\", \"무용\", \"소음\", \"미세\",\n",
    "    \"화석\", \"비재\", \"독물\", \"온실\", \"과열\", \"불균\", \"불안\", \"폐물\", \"에낭\", \"무계\",\n",
    "    \"파적\", \"해로\", \"위협\", \"불안\", \"부실\", \"부정\", \"자형\", \"부실운\", \"오남\", \"무효\",\n",
    "    \"독배\", \"폐증\", \"소과\", \"불친\", \"비도\", \"위험\", \"낭성\", \"비합\", \"대오\", \"수오\",\n",
    "    \"토오\", \"소공\", \"오물\", \"해오\", \"기악\", \"불투\", \"산폐\", \"산파\", \"자파\", \"대질\",\n",
    "    \"자남\", \"화출\", \"유해\", \"폐수\", \"악취\", \"기유\", \"생파\", \"피해\", \"쓰범\", \"미폭\",\n",
    "    \"독폐\", \"불법\", \"불광\", \"비위\", \"무책\", \"폐부\", \"온과\", \"온난\", \"공해\", \"규미\",\n",
    "    \"피증\", \"재불\", \"무추\", \"쓰문\", \"도열\", \"온변\", \"위기\", \"생행\", \"불합\", \"비불\",\n",
    "    \"부퇴\", \"폐악\", \"불폐\", \"비낭\", \"비오\", \"과배\", \"부오\", \"비폐\", \"지오\", \"산오\"\n",
    "]\n",
    "\n",
    "# 추가 단어\n",
    "additional_positive_words = [\"신뢰\", \"품질\"]\n",
    "additional_negative_words = [\"불만\", \"문제\"]\n",
    "\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "# ── 2. 데이터 전처리 함수 ──\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 구두점 제거\n",
    "    return text\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"텍스트 내 긍정/부정 단어 등장 횟수를 비교하여 점수 반환\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# ── 3. CSV 데이터 로드 및 전처리 ──\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"데이터셋 컬럼:\", df.columns)\n",
    "\n",
    "# description 컬럼 전처리 및 lexicon 점수 계산\n",
    "df[\"description_clean\"] = df[\"description\"].apply(clean_text)\n",
    "df[\"lex_score\"] = df[\"description_clean\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# ── 4. SentenceTransformer 임베딩 계산 ──\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 모든 문장에 대해 임베딩 계산 (전처리된 텍스트 사용)\n",
    "descriptions = df[\"description_clean\"].tolist()\n",
    "embeddings = model.encode(descriptions)\n",
    "\n",
    "# ── 5. 특징 결합: 임베딩 + lexicon 점수 ──\n",
    "# lexicon 점수는 스칼라이므로 차원 맞추기 위해 reshape\n",
    "lex_scores = df[\"lex_score\"].values.reshape(-1, 1)\n",
    "# 예: 임베딩 차원이 (N, D)일 때 최종 특징 차원은 (N, D+1)\n",
    "X_combined = np.hstack([embeddings, lex_scores])\n",
    "y = df[\"PN\"].values  # 정답 라벨\n",
    "\n",
    "# ── 6. SVM 분류기 하이퍼파라미터 튜닝 (GridSearchCV) ──\n",
    "svm_clf = SVC(kernel='linear', probability=True)\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_combined, y)\n",
    "\n",
    "print(\"최적의 파라미터:\", grid_search.best_params_)\n",
    "print(\"그리드 서치 최고 정확도: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "# 최적의 분류기로 최종 학습\n",
    "svm_clf_final = grid_search.best_estimator_\n",
    "\n",
    "# ── 7. 학습 데이터 평가 ──\n",
    "pred_train = svm_clf_final.predict(X_combined)\n",
    "print(\"== SVM 분류기 평가 (학습 데이터) ==\")\n",
    "print(\"전체 정확도: {:.2f}%\".format(accuracy_score(y, pred_train) * 100))\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(y, pred_train))\n",
    "print(\"혼동 행렬:\")\n",
    "print(confusion_matrix(y, pred_train))\n",
    "\n",
    "# ── 8. 예측 함수 정의 ──\n",
    "def predict_sentiment(sentence, use_combined=True):\n",
    "    sentence_clean = clean_text(sentence)\n",
    "    emb = model.encode([sentence_clean])\n",
    "    if use_combined:\n",
    "        lex_score = np.array([[lexicon_score(sentence_clean, positive_words, negative_words)]])\n",
    "        feature = np.hstack([emb, lex_score])\n",
    "        pred = svm_clf_final.predict(feature)\n",
    "    else:\n",
    "        pred = svm_clf_final.predict(emb)\n",
    "    return int(pred[0])\n",
    "\n",
    "# ── 9. 테스트 예시 ──\n",
    "test_sentence = \"대중교통을 탄다\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"테스트 문장:\", test_sentence)\n",
    "print(\"예측 결과:\", result)\n",
    "\n",
    "# 결과 CSV 파일로 저장 (예: descriptions_with_lexicon.csv)\n",
    "df.to_csv(\"./bulkdata/descriptions_with_lexicon.csv\", index=False)\n",
    "\n",
    "print(\"적용 완료: descriptions_with_lexicon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2290c8ec-947c-46ec-bba3-c4d5df5654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "positive_words_100 = [\n",
    "    \"설정\",\n",
    "    \"필요한\",\n",
    "    \"모아서\",\n",
    "    \"돌린다\",\n",
    "    \"뽑는다\",\n",
    "    \"뚜껑 덮다\",\n",
    "    \"낮추다\",\n",
    "    \"점검\",\n",
    "    \"교체한다\",\n",
    "    \"꺼둔다\",\n",
    "    \"잠근다\",\n",
    "    \"다회용\",\n",
    "    \"조리한다\",\n",
    "    \"자전거\",\n",
    "    \"걷기\",\n",
    "    \"양면\",\n",
    "    \"감소\",\n",
    "    \"줄인다\",\n",
    "    \"대중교통\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"형광등\",\n",
    "    \"유지한다\",\n",
    "    \"전기밥솥\",\n",
    "    \"보온\",\n",
    "    \"하루 종일\",\n",
    "    \"켜둔다\",\n",
    "    \"음식물 찌꺼기\",\n",
    "    \"씻어보낸다\",\n",
    "    \"세제\",\n",
    "    \"걸레\",\n",
    "    \"흐르는 물\",\n",
    "    \"헹군다\",\n",
    "    \"물티슈\",\n",
    "    \"기저귀\",\n",
    "    \"보일러\",\n",
    "    \"틀어놓는다\",\n",
    "    \"외출\",\n",
    "    \"가스레인지\",\n",
    "    \"불꽃\",\n",
    "    \"퍼지다\",\n",
    "    \"매일\",\n",
    "    \"냉장고 문\",\n",
    "    \"연다\",\n",
    "    \"에어컨\",\n",
    "    \"낮게 유지\",\n",
    "    \"컴퓨터\",\n",
    "    \"항상 켜둔다\",\n",
    "    \"최대 불\",\n",
    "    \"끓인다\",\n",
    "    \"가스\",\n",
    "    \"켜둔\",\n",
    "    \"오랜 시간\",\n",
    "    \"난방\",\n",
    "    \"목욕탕\",\n",
    "    \"채운다\",\n",
    "    \"많이 사용한다\",\n",
    "    \"계속 틀어둔다\",\n",
    "    \"창문\",\n",
    "    \"열어둔다\",\n",
    "    \"집 전체\",\n",
    "    \"난방한다\",\n",
    "    \"난방기\",\n",
    "    \"최고 온도\",\n",
    "    \"설정한다\",\n",
    "    \"일회용\",\n",
    "    \"플라스틱\",\n",
    "    \"포장\",\n",
    "    \"구매한다\",\n",
    "    \"비닐봉지\",\n",
    "    \"넉넉히 만든다\",\n",
    "    \"음식\",\n",
    "    \"버린다\",\n",
    "    \"보관하지 않는다\",\n",
    "    \"차\",\n",
    "    \"자주 이용한다\",\n",
    "    \"자동차\",\n",
    "    \"이용한다\",\n",
    "    \"이용하지 않는다\",\n",
    "    \"종이\",\n",
    "    \"일회용\",\n",
    "    \"한쪽 면\",\n",
    "    \"인쇄한다\",\n",
    "    \"주차 공간\",\n",
    "    \"부족\",\n",
    "    \"심화시킨다\",\n",
    "    \"도로 확장\",\n",
    "    \"유발한다\",\n",
    "    \"환경 파괴\",\n",
    "    \"화석 연료\",\n",
    "    \"사용\",\n",
    "    \"강제한다\",\n",
    "    \"지구 온난화\",\n",
    "    \"가속화한다\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f4e4195-ddb8-43d0-9559-15f7df5e576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 컬럼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "최적의 파라미터: {'C': 1}\n",
      "그리드 서치 최고 정확도: 89.09%\n",
      "== SVM 분류기 평가 (학습 데이터) ==\n",
      "전체 정확도: 90.91%\n",
      "분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.94      0.93        35\n",
      "           1       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.91        55\n",
      "   macro avg       0.91      0.90      0.90        55\n",
      "weighted avg       0.91      0.91      0.91        55\n",
      "\n",
      "혼동 행렬:\n",
      "[[33  2]\n",
      " [ 3 17]]\n",
      "테스트 문장: 대중교통을 탄다\n",
      "예측 결과: 1\n",
      "적용 완료: descriptions_with_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "# 추가 단어\n",
    "additional_positive_words = [\"신뢰\", \"품질\"]\n",
    "additional_negative_words = [\"불만\", \"문제\"]\n",
    "\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "# ── 2. 데이터 전처리 함수 ──\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 구두점 제거\n",
    "    return text\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"텍스트 내 긍정/부정 단어 등장 횟수를 비교하여 점수 반환\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# ── 3. CSV 데이터 로드 및 전처리 ──\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"데이터셋 컬럼:\", df.columns)\n",
    "\n",
    "# description 컬럼 전처리 및 lexicon 점수 계산\n",
    "df[\"description_clean\"] = df[\"description\"].apply(clean_text)\n",
    "df[\"lex_score\"] = df[\"description_clean\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# ── 4. SentenceTransformer 임베딩 계산 ──\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 모든 문장에 대해 임베딩 계산 (전처리된 텍스트 사용)\n",
    "descriptions = df[\"description_clean\"].tolist()\n",
    "embeddings = model.encode(descriptions)\n",
    "\n",
    "# ── 5. 특징 결합: 임베딩 + lexicon 점수 ──\n",
    "# lexicon 점수는 스칼라이므로 차원 맞추기 위해 reshape\n",
    "lex_scores = df[\"lex_score\"].values.reshape(-1, 1)\n",
    "# 예: 임베딩 차원이 (N, D)일 때 최종 특징 차원은 (N, D+1)\n",
    "X_combined = np.hstack([embeddings, lex_scores])\n",
    "y = df[\"PN\"].values  # 정답 라벨\n",
    "\n",
    "# ── 6. SVM 분류기 하이퍼파라미터 튜닝 (GridSearchCV) ──\n",
    "svm_clf = SVC(kernel='linear', probability=True)\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_combined, y)\n",
    "\n",
    "print(\"최적의 파라미터:\", grid_search.best_params_)\n",
    "print(\"그리드 서치 최고 정확도: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "# 최적의 분류기로 최종 학습\n",
    "svm_clf_final = grid_search.best_estimator_\n",
    "\n",
    "# ── 7. 학습 데이터 평가 ──\n",
    "pred_train = svm_clf_final.predict(X_combined)\n",
    "print(\"== SVM 분류기 평가 (학습 데이터) ==\")\n",
    "print(\"전체 정확도: {:.2f}%\".format(accuracy_score(y, pred_train) * 100))\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(y, pred_train))\n",
    "print(\"혼동 행렬:\")\n",
    "print(confusion_matrix(y, pred_train))\n",
    "\n",
    "# ── 8. 예측 함수 정의 ──\n",
    "def predict_sentiment(sentence, use_combined=True):\n",
    "    sentence_clean = clean_text(sentence)\n",
    "    emb = model.encode([sentence_clean])\n",
    "    if use_combined:\n",
    "        lex_score = np.array([[lexicon_score(sentence_clean, positive_words, negative_words)]])\n",
    "        feature = np.hstack([emb, lex_score])\n",
    "        pred = svm_clf_final.predict(feature)\n",
    "    else:\n",
    "        pred = svm_clf_final.predict(emb)\n",
    "    return int(pred[0])\n",
    "\n",
    "# ── 9. 테스트 예시 ──\n",
    "test_sentence = \"대중교통을 탄다\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"테스트 문장:\", test_sentence)\n",
    "print(\"예측 결과:\", result)\n",
    "\n",
    "# 결과 CSV 파일로 저장 (예: descriptions_with_lexicon.csv)\n",
    "df.to_csv(\"./bulkdata/descriptions_with_lexicon.csv\", index=False)\n",
    "\n",
    "print(\"적용 완료: descriptions_with_lexicon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc875f-297c-4405-a5d7-6ded60c6642c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
