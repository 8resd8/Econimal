{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fcb4bf2-cd0b-4b25-8159-705e68d3a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV íŒŒì¼ ì €ìž¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./bulkdata/final_descriptions_with_scores.csv\")\n",
    "\n",
    "# ìµœì¢… DataFrameì—ì„œ description ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "final_description_df = df[['description']]\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ìž¥ (íŒŒì¼ ê²½ë¡œëŠ” í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)\n",
    "final_description_df.to_csv(\"./bulkdata/descriptions.csv\", index=False)\n",
    "\n",
    "print(\"CSV íŒŒì¼ ì €ìž¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3cb5f6-dd0c-4773-925c-6fa28b6b5730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            description  PN\n",
      "0         â„ ì—ì–´ì»¨ì„ 26ë„ë¡œ ì„¤ì •í•˜ê³  í•„ìš”í•œ ì‹œê°„ì—ë§Œ ì¼ ë‹¤.   1\n",
      "1             ðŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.  -1\n",
      "2          ðŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.   1\n",
      "3              ðŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.  -1\n",
      "4            ðŸ”Œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì „ìžì œí’ˆì˜ í”ŒëŸ¬ê·¸ë¥¼ ë½‘ì•„ë‘”ë‹¤.   1\n",
      "5          ðŸ¥˜ ìŒì‹ë¬¼ ì°Œêº¼ê¸°ë¥¼ ë¬¼ë¡œ ì”»ì–´ í•˜ìˆ˜êµ¬ì— í˜ë ¤ë³´ë‚¸ë‹¤.  -1\n",
      "6          ðŸ’§ ì„¸ì œë¥¼ ì‚¬ìš©í•œ ê±¸ë ˆë¥¼ íë¥´ëŠ” ë¬¼ë¡œ ì˜¤ëž˜ í—¹êµ°ë‹¤.  -1\n",
      "7       ðŸ’§ ê¸°ë¦„ ë¬»ì€ í›„ë¼ì´íŒ¬ì„ í‚¤ì¹œíƒ€ì›”ë¡œ ë‹¦ì€ í›„ ì„¤ê±°ì§€í•œë‹¤.   1\n",
      "8                 ðŸ—‘ í™”ìž¥ì‹¤ì— ë¬¼í‹°ìŠˆë‚˜ ê¸°ì €ê·€ë¥¼ ë²„ë¦°ë‹¤.  -1\n",
      "9      ðŸ’§ ë‚¨ì€ ì•½ì´ë‚˜ íê¸°ë¬¼ì„ ì•½êµ­ ë˜ëŠ” ì§€ì • ìˆ˜ê±°í•¨ì— ë²„ë¦°ë‹¤.   1\n",
      "10               ðŸ›¢ ìš”ë¦¬ ì¤‘ ëƒ„ë¹„ ëšœê»‘ì„ ë®ê³  ì¡°ë¦¬í•œë‹¤.   1\n",
      "11          ðŸ”¥ ê²¨ìš¸ì²  ë³´ì¼ëŸ¬ë¥¼ í•˜ë£¨ ì¢…ì¼ í‹€ì–´ë†“ê³  ì™¸ì¶œí•œë‹¤.  -1\n",
      "12     ðŸ”¥ ë³´ì¼ëŸ¬ ì˜¨ë„ëŠ” ë‚®ì¶”ê³ , ì˜¨ìˆ˜ëŠ” í•„ìš”í•œ ë§Œí¼ë§Œ ì‚¬ìš©í•œë‹¤.   1\n",
      "13         ðŸ›¢ ê°€ìŠ¤ë ˆì¸ì§€ì— ë¶ˆê½ƒì´ ëƒ„ë¹„ ë°”ê¹¥ìœ¼ë¡œ í¼ì§€ê²Œ ì¼ ë‹¤.   1\n",
      "14  ðŸ›¢ ê°€ìŠ¤ ëˆ„ì¶œ ì ê²€ì„ ì •ê¸°ì ìœ¼ë¡œ í•˜ê³ , ì˜¤ëž˜ëœ í˜¸ìŠ¤ë¥¼ êµì²´í•œë‹¤.   1\n",
      "15         ðŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.   1\n",
      "16            ðŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.  -1\n",
      "17                        ðŸ§º ë§¤ì¼ ì„¸íƒê¸°ë¥¼ ëŒë¦°ë‹¤   1\n",
      "18             ðŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.  -1\n",
      "19                      ðŸ’¡ ë¶ˆí•„ìš”í•œ ì¡°ëª…ì„ êº¼ë‘”ë‹¤.  -1\n",
      "20                      â„ ëƒ‰ìž¥ê³  ë¬¸ì„ ìžì£¼ ì—°ë‹¤.   1\n",
      "21                â„ ì—ì–´ì»¨ ì˜¨ë„ë¥¼ í•­ìƒ ë‚®ê²Œ ìœ ì§€í•œë‹¤.  -1\n",
      "22                       ðŸ”Œ ì»´í“¨í„°ë¥¼ í•­ìƒ ì¼œë‘”ë‹¤.  -1\n",
      "23                   ðŸ›¢ ìš”ë¦¬ ì‹œ ëƒ„ë¹„ ëšœê»‘ì„ ë®ëŠ”ë‹¤.   1\n",
      "24              ðŸ”¥ ë¬¼ì„ ë“ì¼ ë•Œ í•­ìƒ ìµœëŒ€ ë¶ˆë¡œ ë“ì¸ë‹¤.   1\n",
      "25                 ðŸ›¢ ê°€ìŠ¤ë¥¼ ì¼œë‘” ì±„ ë‹¤ë¥¸ ì¼ì„ í•œë‹¤.   1\n",
      "26                   ðŸ›¢ ë§¤ì¼ ì˜¤ëžœ ì‹œê°„ ë‚œë°©ì„ í•œë‹¤.   1\n",
      "27                      ðŸ’§ ì–‘ì¹˜ì§ˆ ì‹œ ë¬¼ì„ ìž ê·¼ë‹¤.   1\n",
      "28                       ðŸ’§ ëª©ìš•íƒ•ì„ ë§¤ì¼ ì±„ìš´ë‹¤.   1\n",
      "29                        ðŸ’§ ë¬¼ì„ ë§Žì´ ì‚¬ìš©í•œë‹¤.   1\n",
      "30               ðŸ’§ ì„¸ì°¨í•  ë•Œ í•­ìƒ ë¬¼ì„ ê³„ì† í‹€ì–´ë‘”ë‹¤.   1\n",
      "31           ðŸ”¥ ë‚œë°©ì„ ì ì ˆížˆ ë‚®ì¶”ê³  ì˜·ì„ ë”°ëœ»í•˜ê²Œ ìž…ëŠ”ë‹¤.   1\n",
      "32                      â„ ì°½ë¬¸ì„ í•­ìƒ ì—´ì–´ ë‘”ë‹¤.   1\n",
      "33                  ðŸ›¢ ì§‘ ì „ì²´ë¥¼ í•˜ë£¨ ì¢…ì¼ ë‚œë°©í•œë‹¤.  -1\n",
      "34               ðŸ›¢ ë‚œë°©ê¸°ë¥¼ í•­ìƒ ìµœê³  ì˜¨ë„ë¡œ ì„¤ì •í•œë‹¤.   1\n",
      "35                      ðŸ—‘ ë‹¤íšŒìš© ë¬¼ë³‘ì„ ì‚¬ìš©í•œë‹¤.   1\n",
      "36                    ðŸ—‘ ì¼íšŒìš© ì»µì„ ìžì£¼ ì‚¬ìš©í•œë‹¤.   1\n",
      "37              ðŸ—‘ í”Œë¼ìŠ¤í‹± í¬ìž¥ëœ ì œí’ˆì„ ìžì£¼ êµ¬ë§¤í•œë‹¤.   1\n",
      "38                     ðŸ—‘ ë¹„ë‹ë´‰ì§€ë¥¼ ìžì£¼ ì‚¬ìš©í•œë‹¤.   1\n",
      "39                       ðŸ—‘ í•„ìš”í•œ ì–‘ë§Œ ì¡°ë¦¬í•œë‹¤.   1\n",
      "40                    ðŸ¥˜ í•­ìƒ ìŒì‹ì„ ë„‰ë„‰ížˆ ë§Œë“ ë‹¤.   1\n",
      "41                        ðŸ¥˜ ìŒì‹ì„ ìžì£¼ ë²„ë¦°ë‹¤.  -1\n",
      "42                  ðŸ¥˜ ë‚¨ì€ ìŒì‹ ë³´ê´€ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.   1\n",
      "43                   ðŸš² ìžì „ê±° ë˜ëŠ” ê±·ê¸°ë¥¼ ì´ìš©í•œë‹¤.   1\n",
      "44                        ðŸš— ì°¨ë¥¼ ìžì£¼ ì´ìš©í•œë‹¤.   1\n",
      "45                  ðŸš— ê°€ê¹Œìš´ ê±°ë¦¬ë„ ìžë™ì°¨ë¡œ ë‹¤ë‹Œë‹¤.   1\n",
      "46                   ðŸš— ëŒ€ì¤‘êµí†µ ì´ìš©ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.   1\n",
      "47                         ðŸ—‘ ì–‘ë©´ ì¸ì‡„ë¥¼ í•œë‹¤.   1\n",
      "48                   ðŸ—‘ ì¢…ì´ë¥¼ ì¼íšŒìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•œë‹¤.   1\n",
      "49                       ðŸ—‘ ì¢…ì´ë¥¼ ë§Žì´ ì‚¬ìš©í•œë‹¤.   1\n",
      "50                        ðŸ—‘ í•œìª½ ë©´ë§Œ ì¸ì‡„í•œë‹¤.   1\n",
      "51          ðŸŒ³ ëŒ€ê¸°ì˜¤ì—¼ ê°ì†Œì™€ ì—ë„ˆì§€ë¥¼ ì¤„ì´ëŠ” íš¨ê³¼ê°€ ìžˆë‹¤.   1\n",
      "52                ðŸš— ì£¼ì°¨ ê³µê°„ ë¶€ì¡± ë¬¸ì œë¥¼ ì‹¬í™”ì‹œí‚¨ë‹¤.   1\n",
      "53      ðŸš— ë„ë¡œ í™•ìž¥ì„ ìœ ë°œí•˜ì—¬ í™˜ê²½ íŒŒê´´ ì›ì¸ì´ ë  ìˆ˜ ìžˆë‹¤.   1\n",
      "54       ðŸ›¢ í™”ì„ ì—°ë£Œ ì‚¬ìš©ì„ ê°•ì œí•´ ì§€êµ¬ ì˜¨ë‚œí™”ë¥¼ ê°€ì†í™”í•œë‹¤.   1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "import io\n",
    "\n",
    "# 1. í•™ìŠµ ë°ì´í„° ìž¬ì •ì˜ (ì‚¬ìš©ìž ë ˆì´ë¸” ë° í”¼ë“œë°±)\n",
    "user_labels_data = \"\"\"\n",
    "â„ ì—ì–´ì»¨ì„ 26ë„ë¡œ ì„¤ì •í•˜ê³  í•„ìš”í•œ ì‹œê°„ì—ë§Œ ì¼ ë‹¤.,1\n",
    "ðŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.,-1\n",
    "ðŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.,1\n",
    "ðŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.,-1\n",
    "ðŸ”Œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì „ìžì œí’ˆì˜ í”ŒëŸ¬ê·¸ë¥¼ ë½‘ì•„ë‘”ë‹¤.,1\n",
    "ðŸ¥˜ ìŒì‹ë¬¼ ì°Œêº¼ê¸°ë¥¼ ë¬¼ë¡œ ì”»ì–´ í•˜ìˆ˜êµ¬ì— í˜ë ¤ë³´ë‚¸ë‹¤.,-1\n",
    "ðŸ’§ ì„¸ì œë¥¼ ì‚¬ìš©í•œ ê±¸ë ˆë¥¼ íë¥´ëŠ” ë¬¼ë¡œ ì˜¤ëž˜ í—¹êµ°ë‹¤.,-1\n",
    "ðŸ’§ ê¸°ë¦„ ë¬»ì€ í›„ë¼ì´íŒ¬ì„ í‚¤ì¹œíƒ€ì›”ë¡œ ë‹¦ì€ í›„ ì„¤ê±°ì§€í•œë‹¤.,1\n",
    "ðŸ—‘ í™”ìž¥ì‹¤ì— ë¬¼í‹°ìŠˆë‚˜ ê¸°ì €ê·€ë¥¼ ë²„ë¦°ë‹¤.,-1\n",
    "ðŸ’§ ë‚¨ì€ ì•½ì´ë‚˜ íê¸°ë¬¼ì„ ì•½êµ­ ë˜ëŠ” ì§€ì • ìˆ˜ê±°í•¨ì— ë²„ë¦°ë‹¤.,1\n",
    "ðŸ›¢ ìš”ë¦¬ ì¤‘ ëƒ„ë¹„ ëšœê»‘ì„ ë®ê³  ì¡°ë¦¬í•œë‹¤.,1\n",
    "ðŸ”¥ ê²¨ìš¸ì²  ë³´ì¼ëŸ¬ë¥¼ í•˜ë£¨ ì¢…ì¼ í‹€ì–´ë†“ê³  ì™¸ì¶œí•œë‹¤.,-1\n",
    "\"\"\"\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆìž„ ìƒì„±\n",
    "user_df = pd.read_csv(io.StringIO(user_labels_data), header=None, names=[\"description\", \"score\"])\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ê²°í•© í›„, NaN ê°’ ì œê±°\n",
    "updated_train_data = updated_train_data.dropna(subset=[\"user_score\"])\n",
    "\n",
    "# 2. TF-IDF ë²¡í„°í™” ë° Ridge íšŒê·€ ëª¨ë¸ ìž¬í•™ìŠµ\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(updated_train_data[\"description\"])\n",
    "y_train = updated_train_data[\"user_score\"].astype(float)\n",
    "\n",
    "reg_model = Ridge()\n",
    "reg_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° ë° ëª¨ë¸ ì˜ˆì¸¡\n",
    "df_reloaded = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# description ì¹¼ëŸ¼ ê¸°ë°˜ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "X_all = vectorizer.transform(df_reloaded[\"description\"])\n",
    "reg_predictions = reg_model.predict(X_all)\n",
    "\n",
    "# ì´ì§„ ê¸/ë¶€ì • íŒë‹¨: 0ë³´ë‹¤ í¬ë©´ 1(ê¸ì •), ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ -1(ë¶€ì •)\n",
    "df_reloaded[\"PN\"] = [1 if score > 0 else -1 for score in reg_predictions]\n",
    "\n",
    "# ê²°ê³¼ ë¯¸ë¦¬ ë³´ê¸°\n",
    "print(df_reloaded.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b07e429-2e27-463c-8b62-0fdd032bdc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì ìš© ì™„ë£Œ: descriptions_with_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ê¸ì • ë‹¨ì–´ 100ê°œ (ê° ë‹¨ì–´ëŠ” ìµœëŒ€ 3ê¸€ìž)\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ìž¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ìž¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìžì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ìž¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ìž\", \"ì •ì±…\",\n",
    "    \"ìžì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìžë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëžŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ìž¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìží™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìžê²½\", \"ì¹œìž\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìžì§€\", \"ì—ê²½\", \"ì—ì •\", \"ìž¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìží•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "# ë¶€ì • ë‹¨ì–´ 100ê°œ (ê° ë‹¨ì–´ëŠ” ìµœëŒ€ 3ê¸€ìž)\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìžë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ìž¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìží˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìžíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìžë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ìž¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë‚´ì— í¬í•¨ëœ ê¸ì • ë‹¨ì–´ì™€ ë¶€ì • ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œì— ë§žê²Œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# ê° ë¬¸ìž¥ì— ëŒ€í•´ ì‚¬ì „ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (lex_score)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words_100, negative_words_100))\n",
    "\n",
    "# ê²°ê³¼ CSV íŒŒì¼ë¡œ ì €ìž¥ (ì˜ˆ: descriptions_with_lexicon.csv)\n",
    "df.to_csv(\"./bulkdata/descriptions_with_lexicon.csv\", index=False)\n",
    "\n",
    "print(\"ì ìš© ì™„ë£Œ: descriptions_with_lexicon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b455f0-cf8a-441d-b895-1716716f4bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\SSAFY\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SSAFY\\.cache\\huggingface\\hub\\models--sentence-transformers--distiluse-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            description  lex_score  PN_final\n",
      "0         â„ ì—ì–´ì»¨ì„ 26ë„ë¡œ ì„¤ì •í•˜ê³  í•„ìš”í•œ ì‹œê°„ì—ë§Œ ì¼ ë‹¤.          0         1\n",
      "1             ðŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.          0         1\n",
      "2          ðŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.          0         1\n",
      "3              ðŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.          0         1\n",
      "4            ðŸ”Œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì „ìžì œí’ˆì˜ í”ŒëŸ¬ê·¸ë¥¼ ë½‘ì•„ë‘”ë‹¤.          0        -1\n",
      "5          ðŸ¥˜ ìŒì‹ë¬¼ ì°Œêº¼ê¸°ë¥¼ ë¬¼ë¡œ ì”»ì–´ í•˜ìˆ˜êµ¬ì— í˜ë ¤ë³´ë‚¸ë‹¤.          0        -1\n",
      "6          ðŸ’§ ì„¸ì œë¥¼ ì‚¬ìš©í•œ ê±¸ë ˆë¥¼ íë¥´ëŠ” ë¬¼ë¡œ ì˜¤ëž˜ í—¹êµ°ë‹¤.          0         1\n",
      "7       ðŸ’§ ê¸°ë¦„ ë¬»ì€ í›„ë¼ì´íŒ¬ì„ í‚¤ì¹œíƒ€ì›”ë¡œ ë‹¦ì€ í›„ ì„¤ê±°ì§€í•œë‹¤.          0         1\n",
      "8                 ðŸ—‘ í™”ìž¥ì‹¤ì— ë¬¼í‹°ìŠˆë‚˜ ê¸°ì €ê·€ë¥¼ ë²„ë¦°ë‹¤.          0        -1\n",
      "9      ðŸ’§ ë‚¨ì€ ì•½ì´ë‚˜ íê¸°ë¬¼ì„ ì•½êµ­ ë˜ëŠ” ì§€ì • ìˆ˜ê±°í•¨ì— ë²„ë¦°ë‹¤.         -1        -1\n",
      "10               ðŸ›¢ ìš”ë¦¬ ì¤‘ ëƒ„ë¹„ ëšœê»‘ì„ ë®ê³  ì¡°ë¦¬í•œë‹¤.          0        -1\n",
      "11          ðŸ”¥ ê²¨ìš¸ì²  ë³´ì¼ëŸ¬ë¥¼ í•˜ë£¨ ì¢…ì¼ í‹€ì–´ë†“ê³  ì™¸ì¶œí•œë‹¤.          0        -1\n",
      "12     ðŸ”¥ ë³´ì¼ëŸ¬ ì˜¨ë„ëŠ” ë‚®ì¶”ê³ , ì˜¨ìˆ˜ëŠ” í•„ìš”í•œ ë§Œí¼ë§Œ ì‚¬ìš©í•œë‹¤.          0         1\n",
      "13         ðŸ›¢ ê°€ìŠ¤ë ˆì¸ì§€ì— ë¶ˆê½ƒì´ ëƒ„ë¹„ ë°”ê¹¥ìœ¼ë¡œ í¼ì§€ê²Œ ì¼ ë‹¤.          0        -1\n",
      "14  ðŸ›¢ ê°€ìŠ¤ ëˆ„ì¶œ ì ê²€ì„ ì •ê¸°ì ìœ¼ë¡œ í•˜ê³ , ì˜¤ëž˜ëœ í˜¸ìŠ¤ë¥¼ êµì²´í•œë‹¤.          0         1\n",
      "15         ðŸ§º ì„¸íƒë¬¼ì„ ëª¨ì•„ì„œ ì„¸íƒê¸°ë¥¼ ì£¼ 2~3íšŒë§Œ ëŒë¦°ë‹¤.          0         1\n",
      "16            ðŸ’¡ ì§‘ ì•ˆì˜ ëª¨ë“  ì¡°ëª…ì„ í˜•ê´‘ë“±ìœ¼ë¡œ ìœ ì§€í•œë‹¤.          0         1\n",
      "17                        ðŸ§º ë§¤ì¼ ì„¸íƒê¸°ë¥¼ ëŒë¦°ë‹¤          0         1\n",
      "18             ðŸ”Œ ì „ê¸°ë°¥ì†¥ ë³´ì˜¨ ê¸°ëŠ¥ì„ í•˜ë£¨ ì¢…ì¼ ì¼œë‘”ë‹¤.          0         1\n",
      "19                      ðŸ’¡ ë¶ˆí•„ìš”í•œ ì¡°ëª…ì„ êº¼ë‘”ë‹¤.         -1        -1\n",
      "ìµœì¢… ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: descriptions_with_PN_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ê¸ì • ë‹¨ì–´ 100ê°œ (ê° ë‹¨ì–´ëŠ” ìµœëŒ€ 3ê¸€ìž)\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ìž¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ìž¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìžì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ìž¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ìž\", \"ì •ì±…\",\n",
    "    \"ìžì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìžë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëžŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ìž¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìží™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìžê²½\", \"ì¹œìž\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìžì§€\", \"ì—ê²½\", \"ì—ì •\", \"ìž¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìží•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "# ë¶€ì • ë‹¨ì–´ 100ê°œ (ê° ë‹¨ì–´ëŠ” ìµœëŒ€ 3ê¸€ìž)\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìžë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ìž¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìží˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìžíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìžë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ìž¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë‚´ ê¸ì •/ë¶€ì • ë‹¨ì–´ ê°œìˆ˜ë¥¼ ì„¸ì–´ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œì— ë§žê²Œ ìˆ˜ì •)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions.csv\")\n",
    "\n",
    "# ê° ë¬¸ìž¥ì— ëŒ€í•´ ì‚¬ì „ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (lex_score)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words_100, negative_words_100))\n",
    "\n",
    "# sentence-transformers ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ ì‚¬ìš©)\n",
    "# í•œêµ­ì–´ì˜ ê²½ìš°, 'distiluse-base-multilingual-cased' ë˜ëŠ” 'paraphrase-multilingual-MiniLM-L12-v2'ì™€ ê°™ì´ ë‹¤êµ­ì–´ ëª¨ë¸ì„ ê¶Œìž¥í•©ë‹ˆë‹¤.\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ê¸ì • ë‹¨ì–´ ë° ë¶€ì • ë‹¨ì–´ì˜ ìž„ë² ë”© ê³„ì‚° (ëª¨ë¸ì— ì¡´ìž¬í•˜ëŠ” ë‹¨ì–´ ëª¨ë‘ ì‚¬ìš©)\n",
    "pos_embs = [model.encode(word) for word in positive_words_100]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words_100]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"ë¬¸ìž¥ì˜ ìž„ë² ë”©ì„ êµ¬í•œ í›„, ê¸ì •/ë¶€ì • í‰ê·  ìž„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    # ì‚¬ì „ ê¸°ë°˜ ì ìˆ˜ê°€ 0ì´ ì•„ë‹ˆë©´ ê·¸ ê²°ê³¼ ì‚¬ìš©, 0ì´ë©´ sentence-transformers ê¸°ë°˜ ìœ ì‚¬ë„ íŒë‹¨\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# ìµœì¢… ë¼ë²¨(PN_final) ë¶€ì—¬\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸ (ìƒìœ„ 20í–‰ ì¶œë ¥)\n",
    "print(df.head(20))\n",
    "\n",
    "# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ìž¥\n",
    "df.to_csv(\"./bulkdata/descriptions_with_PN_final.csv\", index=False)\n",
    "print(\"ìµœì¢… ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: descriptions_with_PN_final.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e934b452-9c2d-4f96-ac4f-1658206629c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "ì˜ˆì¸¡ ê²°ê³¼: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. CSV íŒŒì¼ì—ì„œ íŠ¸ë ˆì´ë‹ ë°ì´í„° ë¡œë“œ\n",
    "#    íŒŒì¼ì—ëŠ” 'description'ê³¼ 'PN' ì»¬ëŸ¼ì´ ìžˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "df_train = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df_train.columns)\n",
    "\n",
    "# 2. ì‚¬ì „í•™ìŠµëœ SentenceTransformer ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì›)\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# 3. í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ìž„ë² ë”© ìƒì„±\n",
    "#    ê° ë¬¸ìž¥ì„ ëª¨ë¸ì„ ì´ìš©í•´ ë²¡í„°í™”í•©ë‹ˆë‹¤.\n",
    "descriptions = df_train['description'].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df_train['PN'].values  # ë¼ë²¨: 1 (ê¸ì •), -1 (ë¶€ì •)\n",
    "\n",
    "# 4. ê°„ë‹¨í•œ ë¶„ë¥˜ê¸°(ë¡œì§€ìŠ¤í‹± íšŒê·€) í•™ìŠµ\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. ìƒˆë¡œìš´ ë¬¸ìž¥ì„ ìž…ë ¥ë°›ì•„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    ìž…ë ¥ëœ ë¬¸ìž¥ì— ëŒ€í•´ ê¸ì •(1) ë˜ëŠ” ë¶€ì •(-1) ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): ì˜ˆì¸¡í•  ë¬¸ìž¥\n",
    "        \n",
    "    Returns:\n",
    "        int: ì˜ˆì¸¡ëœ ë¼ë²¨ (1 ë˜ëŠ” -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# ì˜ˆì‹œ: í…ŒìŠ¤íŠ¸ ë¬¸ìž¥ ì˜ˆì¸¡\n",
    "test_sentence = \"ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "342ffab0-b5f1-4cf3-9b21-16ae9510f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\n",
      "ì „ì²´ ì •í™•ë„: 56.36%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.57      0.62        35\n",
      "           1       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.56      0.56      0.55        55\n",
      "weighted avg       0.59      0.56      0.57        55\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬:\n",
      "[[20 15]\n",
      " [ 9 11]]\n",
      "== ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\n",
      "ì „ì²´ ì •í™•ë„: 67.27%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      1.00      0.80        35\n",
      "           1       1.00      0.10      0.18        20\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.83      0.55      0.49        55\n",
      "weighted avg       0.78      0.67      0.57        55\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë¬¸ìž¥: ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\n",
      "ì˜ˆì¸¡ ê²°ê³¼: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# â”€â”€ 1. ê¸°ë³¸ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ â”€â”€\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ìž¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ìž¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìžì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ìž¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ìž\", \"ì •ì±…\",\n",
    "    \"ìžì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìžë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëžŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ìž¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìží™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìžê²½\", \"ì¹œìž\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìžì§€\", \"ì—ê²½\", \"ì—ì •\", \"ìž¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìží•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìžë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ìž¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìží˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìžíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìžë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ìž¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "# â”€â”€ 2. ì¶”ê°€ ë‹¨ì–´ ì„¤ì • (ì‚¬ìš©ìž ì •ì˜) â”€â”€\n",
    "additional_positive_words = [\"ì‹ ë¢°\", \"í’ˆì§ˆ\"]   # ì¶”ê°€ ê¸ì • ë‹¨ì–´\n",
    "additional_negative_words = [\"ë¶ˆë§Œ\", \"ë¬¸ì œ\"]     # ì¶”ê°€ ë¶€ì • ë‹¨ì–´\n",
    "\n",
    "# ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ì™€ ì¶”ê°€ ë‹¨ì–´ë¥¼ í•©ì¹¨\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë‚´ ê¸ì • ë‹¨ì–´ì™€ ë¶€ì • ë‹¨ì–´ ë“±ìž¥ íšŸìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# â”€â”€ 3. CSV íŒŒì¼ ë¡œë“œ ë° ì •ë‹µ ë¼ë²¨(PN) í™œìš© â”€â”€\n",
    "# íŒŒì¼ì—ëŠ” ìµœì†Œí•œ 'description'ê³¼ ì •ë‹µ ë¼ë²¨ì¸ 'PN' ì»¬ëŸ¼ì´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df.columns)\n",
    "\n",
    "# lexicon ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (ì¶”ê°€ ë‹¨ì–´ê¹Œì§€ ë°˜ì˜)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# â”€â”€ 4. SentenceTransformer ëª¨ë¸ ë¡œë“œ ë° ìž„ë² ë”© ê¸°ë°˜ ë¼ë²¨ ì‚°ì¶œ â”€â”€\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ê¸ì •/ë¶€ì • ë‹¨ì–´ë“¤ì˜ ìž„ë² ë”© í‰ê·  ê³„ì‚° (ëª¨ë“  ë‹¨ì–´ ì‚¬ìš©)\n",
    "pos_embs = [model.encode(word) for word in positive_words]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"\n",
    "    ë¬¸ìž¥ì˜ ìž„ë² ë”©ì„ êµ¬í•œ í›„, ê¸ì •/ë¶€ì • í‰ê·  ìž„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    \"\"\"\n",
    "    lexicon ê¸°ë°˜ ì ìˆ˜ê°€ 0ì´ ì•„ë‹ˆë©´ í•´ë‹¹ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ê³ , 0ì´ë©´ ìž„ë² ë”© ìœ ì‚¬ë„ ë¹„êµ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# lexicon ë° ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë¼ë²¨(PN_final) ì‚°ì¶œ\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# â”€â”€ 5. ì •ë‹µ(PN)ê³¼ ìš°ë¦¬ ë°©ë²•(PN_final) ë¹„êµ í‰ê°€ â”€â”€\n",
    "print(\"== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(df[\"PN\"], df[\"PN_final\"])*100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(df[\"PN\"], df[\"PN_final\"]))\n",
    "print(\"í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(df[\"PN\"], df[\"PN_final\"]))\n",
    "\n",
    "# â”€â”€ 6. ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ê¸° í•™ìŠµ (ë¬¸ìž¥ ìž„ë² ë”©ì„ ì´ìš©, ì •ë‹µ ë¼ë²¨ PN ì‚¬ìš©) â”€â”€\n",
    "descriptions = df[\"description\"].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df[\"PN\"].values  # ì •ë‹µ ë¼ë²¨ ì‚¬ìš©\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# â”€â”€ 7. ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„°ì—ì„œ ì˜ˆì‹œ) â”€â”€\n",
    "pred_train = clf.predict(X_train)\n",
    "print(\"== ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(y_train, pred_train)*100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_train, pred_train))\n",
    "\n",
    "# â”€â”€ 8. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ â”€â”€\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    ìž…ë ¥ëœ ë¬¸ìž¥ì— ëŒ€í•´ í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¥¼ ì´ìš©í•´ ê°ì„± ë¼ë²¨(1: ê¸ì •, -1: ë¶€ì •)ì„ ì˜ˆì¸¡í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): ì˜ˆì¸¡í•  ë¬¸ìž¥\n",
    "        \n",
    "    Returns:\n",
    "        int: ì˜ˆì¸¡ëœ ê°ì„± ë¼ë²¨ (1 ë˜ëŠ” -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# â”€â”€ 9. í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ â”€â”€\n",
    "test_sentence = \"ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë¬¸ìž¥:\", test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f96365-1c1d-4a52-9cd4-5e47791dd531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\n",
      "ì „ì²´ ì •í™•ë„: 56.36%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.57      0.62        35\n",
      "           1       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.56      0.56      0.55        55\n",
      "weighted avg       0.59      0.56      0.57        55\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬:\n",
      "[[20 15]\n",
      " [ 9 11]]\n",
      "== SVM ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\n",
      "ì „ì²´ ì •í™•ë„: 67.27%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      1.00      0.80        35\n",
      "           1       1.00      0.10      0.18        20\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.83      0.55      0.49        55\n",
      "weighted avg       0.78      0.67      0.57        55\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë¬¸ìž¥: ëŒ€ì¤‘êµí†µì„ íƒ€ì§€ ì•ŠëŠ”ë‹¤\n",
      "ì˜ˆì¸¡ ê²°ê³¼: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# â”€â”€ 1. ê¸°ë³¸ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ â”€â”€\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ìž¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ìž¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìžì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ìž¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ìž\", \"ì •ì±…\",\n",
    "    \"ìžì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìžë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëžŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ìž¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìží™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìžê²½\", \"ì¹œìž\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìžì§€\", \"ì—ê²½\", \"ì—ì •\", \"ìž¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìží•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìžë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ìž¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìží˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìžíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìžë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ìž¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "# â”€â”€ 2. ì¶”ê°€ ë‹¨ì–´ ì„¤ì • (ì‚¬ìš©ìž ì •ì˜) â”€â”€\n",
    "additional_positive_words = [\"ì‹ ë¢°\", \"í’ˆì§ˆ\"]   # ì¶”ê°€ ê¸ì • ë‹¨ì–´\n",
    "additional_negative_words = [\"ë¶ˆë§Œ\", \"ë¬¸ì œ\"]     # ì¶”ê°€ ë¶€ì • ë‹¨ì–´\n",
    "\n",
    "# ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ì™€ ì¶”ê°€ ë‹¨ì–´ë¥¼ í•©ì¹¨\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë‚´ ê¸ì • ë‹¨ì–´ì™€ ë¶€ì • ë‹¨ì–´ ë“±ìž¥ íšŸìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# â”€â”€ 3. CSV íŒŒì¼ ë¡œë“œ ë° ì •ë‹µ ë¼ë²¨(PN) í™œìš© â”€â”€\n",
    "# íŒŒì¼ì—ëŠ” 'description'ê³¼ ì •ë‹µ ë¼ë²¨ 'PN' ì»¬ëŸ¼ì´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df.columns)\n",
    "\n",
    "# lexicon ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (ì¶”ê°€ ë‹¨ì–´ê¹Œì§€ ë°˜ì˜)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# â”€â”€ 4. SentenceTransformer ëª¨ë¸ ë¡œë“œ ë° ìž„ë² ë”© ê¸°ë°˜ ë¼ë²¨ ì‚°ì¶œ â”€â”€\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ê¸ì •/ë¶€ì • ë‹¨ì–´ë“¤ì˜ ìž„ë² ë”© í‰ê·  ê³„ì‚° (ëª¨ë“  ë‹¨ì–´ ì‚¬ìš©)\n",
    "pos_embs = [model.encode(word) for word in positive_words]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"\n",
    "    ë¬¸ìž¥ì˜ ìž„ë² ë”©ì„ êµ¬í•œ í›„, ê¸ì •/ë¶€ì • í‰ê·  ìž„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    \"\"\"\n",
    "    lexicon ê¸°ë°˜ ì ìˆ˜ê°€ 0ì´ ì•„ë‹ˆë©´ í•´ë‹¹ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ê³ , 0ì´ë©´ ìž„ë² ë”© ìœ ì‚¬ë„ ë¹„êµ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# lexicon ë° ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë¼ë²¨(PN_final) ì‚°ì¶œ\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# â”€â”€ 5. ì •ë‹µ(PN)ê³¼ ìš°ë¦¬ ë°©ë²•(PN_final) ë¹„êµ í‰ê°€ â”€â”€\n",
    "print(\"== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(df[\"PN\"], df[\"PN_final\"]) * 100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(df[\"PN\"], df[\"PN_final\"]))\n",
    "print(\"í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(df[\"PN\"], df[\"PN_final\"]))\n",
    "\n",
    "# â”€â”€ 6. SVM ë¶„ë¥˜ê¸° í•™ìŠµ (ë¬¸ìž¥ ìž„ë² ë”©ì„ ì´ìš©, ì •ë‹µ ë¼ë²¨ PN ì‚¬ìš©) â”€â”€\n",
    "descriptions = df[\"description\"].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df[\"PN\"].values  # ì •ë‹µ ë¼ë²¨ ì‚¬ìš©\n",
    "\n",
    "# SVM ë¶„ë¥˜ê¸° (ì—¬ê¸°ì„œëŠ” Linear kernel ì‚¬ìš©)\n",
    "svm_clf = SVC(kernel='linear', probability=True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# â”€â”€ 7. ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„° í‰ê°€) â”€â”€\n",
    "pred_train = svm_clf.predict(X_train)\n",
    "print(\"== SVM ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(y_train, pred_train) * 100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_train, pred_train))\n",
    "\n",
    "# â”€â”€ 8. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ â”€â”€\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    ìž…ë ¥ëœ ë¬¸ìž¥ì— ëŒ€í•´ í•™ìŠµëœ SVM ë¶„ë¥˜ê¸°ë¥¼ ì´ìš©í•´ ê°ì„± ë¼ë²¨(1: ê¸ì •, -1: ë¶€ì •)ì„ ì˜ˆì¸¡í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): ì˜ˆì¸¡í•  ë¬¸ìž¥\n",
    "        \n",
    "    Returns:\n",
    "        int: ì˜ˆì¸¡ëœ ê°ì„± ë¼ë²¨ (1 ë˜ëŠ” -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = svm_clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# â”€â”€ 9. í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ â”€â”€\n",
    "test_sentence = \"ëŒ€ì¤‘êµí†µì„ íƒ€ì§€ ì•ŠëŠ”ë‹¤\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë¬¸ìž¥:\", test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5ca12c-8886-4ec7-90f2-43a7c08d614f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (25) must match the size of tensor b (16) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m num_epochs = \u001b[32m3\u001b[39m\n\u001b[32m     58\u001b[39m warmup_steps = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloader) * num_epochs * \u001b[32m0.1\u001b[39m)  \u001b[38;5;66;03m# ì•½ 10%ì˜ warmup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# fine tuning í›„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ (ë¶„ë¥˜ logits ìƒì„±)\u001b[39;00m\n\u001b[32m     67\u001b[39m example_sentence = \u001b[33m\"\u001b[39m\u001b[33mì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\fit_mixin.py:408\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[39m\n\u001b[32m    405\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint directory does not exist or is not a directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    406\u001b[39m         resume_from_checkpoint = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\trainer.py:2556\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2549\u001b[39m context = (\n\u001b[32m   2550\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2553\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2554\u001b[39m )\n\u001b[32m   2555\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2556\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2559\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2561\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2562\u001b[39m ):\n\u001b[32m   2563\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2564\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\trainer.py:3718\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3715\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3717\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3718\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3720\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3722\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3723\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3724\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\trainer.py:407\u001b[39m, in \u001b[36mSentenceTransformerTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    401\u001b[39m     model == \u001b[38;5;28mself\u001b[39m.model_wrapped\n\u001b[32m    402\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m model != \u001b[38;5;28mself\u001b[39m.model  \u001b[38;5;66;03m# Only if the model is wrapped\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(loss_fn, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Only if the loss stores the model\u001b[39;00m\n\u001b[32m    404\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m loss_fn.model != model  \u001b[38;5;66;03m# Only if the wrapped model is not already stored\u001b[39;00m\n\u001b[32m    405\u001b[39m ):\n\u001b[32m    406\u001b[39m     loss_fn = \u001b[38;5;28mself\u001b[39m.override_model_in_loss(loss_fn, model)\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n\u001b[32m    409\u001b[39m     \u001b[38;5;66;03m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[39;00m\n\u001b[32m    410\u001b[39m     \u001b[38;5;66;03m# However, Sentence Transformer losses do not return outputs, so we return an empty dictionary.\u001b[39;00m\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# This does not result in any problems, as the SentenceTransformerTrainingArguments sets\u001b[39;00m\n\u001b[32m    412\u001b[39m     \u001b[38;5;66;03m# `prediction_loss_only=True` which means that the output is not used.\u001b[39;00m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mClassificationLoss.forward\u001b[39m\u001b[34m(self, sentence_features, labels)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     49\u001b[39m     batched_features = sentence_features\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss_fct(logits, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\SentenceTransformer.py:619\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor], **kwargs) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module_name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.named_children():\n\u001b[32m    622\u001b[39m         module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m output_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m output_tokens = output_states[\u001b[32m0\u001b[39m]\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:784\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    782\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_flash_attention_2:\n\u001b[32m    787\u001b[39m     attention_mask = attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:131\u001b[39m, in \u001b[36mEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, input_embeds)\u001b[39m\n\u001b[32m    127\u001b[39m     position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m).expand_as(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length)\u001b[39;00m\n\u001b[32m    129\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.position_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m embeddings = \u001b[43minput_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m    132\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.LayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m    133\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.dropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (25) must match the size of tensor b (16) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import InputExample, SentenceTransformer, SentencesDataset\n",
    "from sentence_transformers.models import Transformer, Pooling, Dense\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ (ì˜ˆ: 'description'ê³¼ ì •ë‹µ ë¼ë²¨ 'PN'ì´ í¬í•¨ëœ íŒŒì¼)\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "\n",
    "# InputExample ê°ì²´ ìƒì„± (ì •ë‹µ ë¼ë²¨ì´ -1, 1ì¸ ê²½ìš°, ë¶„ë¥˜ë¥¼ ìœ„í•´ 0, 1ë¡œ ë³€í™˜)\n",
    "train_examples = []\n",
    "for idx, row in df.iterrows():\n",
    "    label = 0 if row['PN'] == -1 else 1\n",
    "    train_examples.append(InputExample(texts=[row['description']], label=label))\n",
    "\n",
    "num_labels = 2  # ë¶€ì •(0), ê¸ì •(1)\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±: Transformer -> Pooling -> Dense(ë¶„ë¥˜ í—¤ë“œ)\n",
    "model_name = 'sentence-transformers/distiluse-base-multilingual-cased'\n",
    "word_embedding_model = Transformer(model_name, max_seq_length=128)\n",
    "pooling_model = Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "classification_head = Dense(\n",
    "    in_features=pooling_model.get_sentence_embedding_dimension(),\n",
    "    out_features=num_labels,\n",
    "    activation_function=nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, classification_head])\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\n",
    "train_dataset = SentencesDataset(train_examples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ë¶„ë¥˜ ì†ì‹¤ í•¨ìˆ˜ (ë°°ì¹˜ ë‚´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜)\n",
    "class ClassificationLoss(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ClassificationLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, sentence_features, labels):\n",
    "        # ë°°ì¹˜ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜\n",
    "        if isinstance(sentence_features, list):\n",
    "            batched_features = {}\n",
    "            for key in sentence_features[0].keys():\n",
    "                batched_features[key] = torch.stack([sf[key] for sf in sentence_features], dim=0)\n",
    "        else:\n",
    "            batched_features = sentence_features\n",
    "\n",
    "        logits = self.model(batched_features)\n",
    "        return self.loss_fct(logits, labels)\n",
    "\n",
    "train_loss = ClassificationLoss(model)\n",
    "\n",
    "# Fine tuning ì‹¤í–‰\n",
    "num_epochs = 3\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # ì•½ 10%ì˜ warmup\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# fine tuning í›„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ (ë¶„ë¥˜ logits ìƒì„±)\n",
    "example_sentence = \"ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\"\n",
    "inputs = model.tokenizer(example_sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(inputs)\n",
    "print(\"Classification logits:\", logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3ebc73-579a-4640-ac87-b2d5c5c93275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(\"Accelerate version:\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88e2a37-13cb-4636-9cde-7b53c9ed817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word_embedding_model.auto_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322b1f0a-7e4e-481d-ae03-bf5816e439ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\n",
      "ì „ì²´ ì •í™•ë„: 56.36%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.57      0.62        35\n",
      "           1       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.56      0.56      0.55        55\n",
      "weighted avg       0.59      0.56      0.57        55\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬:\n",
      "[[20 15]\n",
      " [ 9 11]]\n",
      "== ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\n",
      "ì „ì²´ ì •í™•ë„: 67.27%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      1.00      0.80        35\n",
      "           1       1.00      0.10      0.18        20\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.83      0.55      0.49        55\n",
      "weighted avg       0.78      0.67      0.57        55\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ë¬¸ìž¥: ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\n",
      "ì˜ˆì¸¡ ê²°ê³¼: -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # êµ¬ë‘ì  ì œê±°\n",
    "    return text\n",
    "\n",
    "# â”€â”€ 1. ê¸°ë³¸ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ â”€â”€\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ìž¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ìž¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìžì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ìž¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ìž\", \"ì •ì±…\",\n",
    "    \"ìžì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìžë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëžŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ìž¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìží™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìžê²½\", \"ì¹œìž\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìžì§€\", \"ì—ê²½\", \"ì—ì •\", \"ìž¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìží•©\", \"ë…¹í•©\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìžë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ìž¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìží˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìžíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìžë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ìž¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "# â”€â”€ 2. ì¶”ê°€ ë‹¨ì–´ ì„¤ì • (ì‚¬ìš©ìž ì •ì˜) â”€â”€\n",
    "additional_positive_words = [\"ì‹ ë¢°\", \"í’ˆì§ˆ\"]   # ì¶”ê°€ ê¸ì • ë‹¨ì–´\n",
    "additional_negative_words = [\"ë¶ˆë§Œ\", \"ë¬¸ì œ\"]     # ì¶”ê°€ ë¶€ì • ë‹¨ì–´\n",
    "\n",
    "# ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ì™€ ì¶”ê°€ ë‹¨ì–´ë¥¼ í•©ì¹¨\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë‚´ ê¸ì • ë‹¨ì–´ì™€ ë¶€ì • ë‹¨ì–´ ë“±ìž¥ íšŸìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# â”€â”€ 3. CSV íŒŒì¼ ë¡œë“œ ë° ì •ë‹µ ë¼ë²¨(PN) í™œìš© â”€â”€\n",
    "# íŒŒì¼ì—ëŠ” ìµœì†Œí•œ 'description'ê³¼ ì •ë‹µ ë¼ë²¨ì¸ 'PN' ì»¬ëŸ¼ì´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df.columns)\n",
    "\n",
    "# lexicon ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (ì¶”ê°€ ë‹¨ì–´ê¹Œì§€ ë°˜ì˜)\n",
    "df[\"lex_score\"] = df[\"description\"].apply(clean_text).apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# â”€â”€ 4. SentenceTransformer ëª¨ë¸ ë¡œë“œ ë° ìž„ë² ë”© ê¸°ë°˜ ë¼ë²¨ ì‚°ì¶œ â”€â”€\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ê¸ì •/ë¶€ì • ë‹¨ì–´ë“¤ì˜ ìž„ë² ë”© í‰ê·  ê³„ì‚° (ëª¨ë“  ë‹¨ì–´ ì‚¬ìš©)\n",
    "pos_embs = [model.encode(word) for word in positive_words]\n",
    "avg_pos_emb = np.mean(pos_embs, axis=0) if pos_embs else None\n",
    "\n",
    "neg_embs = [model.encode(word) for word in negative_words]\n",
    "avg_neg_emb = np.mean(neg_embs, axis=0) if neg_embs else None\n",
    "\n",
    "def assign_label_sentence_transformers(text):\n",
    "    \"\"\"\n",
    "    ë¬¸ìž¥ì˜ ìž„ë² ë”©ì„ êµ¬í•œ í›„, ê¸ì •/ë¶€ì • í‰ê·  ìž„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    text_emb = model.encode(text)\n",
    "    pos_sim = cosine_similarity(text_emb.reshape(1, -1), avg_pos_emb.reshape(1, -1))[0][0] if avg_pos_emb is not None else 0\n",
    "    neg_sim = cosine_similarity(text_emb.reshape(1, -1), avg_neg_emb.reshape(1, -1))[0][0] if avg_neg_emb is not None else 0\n",
    "    return 1 if pos_sim > neg_sim else -1\n",
    "\n",
    "def compute_final_label(row):\n",
    "    \"\"\"\n",
    "    lexicon ê¸°ë°˜ ì ìˆ˜ê°€ 0ì´ ì•„ë‹ˆë©´ í•´ë‹¹ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ê³ , 0ì´ë©´ ìž„ë² ë”© ìœ ì‚¬ë„ ë¹„êµ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if row[\"lex_score\"] != 0:\n",
    "        return 1 if row[\"lex_score\"] > 0 else -1\n",
    "    else:\n",
    "        return assign_label_sentence_transformers(row[\"description\"])\n",
    "\n",
    "# lexicon ë° ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë¼ë²¨(PN_final) ì‚°ì¶œ\n",
    "df[\"PN_final\"] = df.apply(compute_final_label, axis=1)\n",
    "\n",
    "# â”€â”€ 5. ì •ë‹µ(PN)ê³¼ ìš°ë¦¬ ë°©ë²•(PN_final) ë¹„êµ í‰ê°€ â”€â”€\n",
    "print(\"== Lexicon/Embedding ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ(PN) ë¹„êµ ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(df[\"PN\"], df[\"PN_final\"])*100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(df[\"PN\"], df[\"PN_final\"]))\n",
    "print(\"í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(df[\"PN\"], df[\"PN_final\"]))\n",
    "\n",
    "# â”€â”€ 6. ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ê¸° í•™ìŠµ (ë¬¸ìž¥ ìž„ë² ë”©ì„ ì´ìš©, ì •ë‹µ ë¼ë²¨ PN ì‚¬ìš©) â”€â”€\n",
    "descriptions = df[\"description\"].tolist()\n",
    "X_train = model.encode(descriptions)\n",
    "y_train = df[\"PN\"].values  # ì •ë‹µ ë¼ë²¨ ì‚¬ìš©\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# â”€â”€ 7. ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„°ì—ì„œ ì˜ˆì‹œ) â”€â”€\n",
    "pred_train = clf.predict(X_train)\n",
    "print(\"== ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„° í‰ê°€) ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(y_train, pred_train)*100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_train, pred_train))\n",
    "\n",
    "# â”€â”€ 8. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ â”€â”€\n",
    "def predict_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    ìž…ë ¥ëœ ë¬¸ìž¥ì— ëŒ€í•´ í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¥¼ ì´ìš©í•´ ê°ì„± ë¼ë²¨(1: ê¸ì •, -1: ë¶€ì •)ì„ ì˜ˆì¸¡í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): ì˜ˆì¸¡í•  ë¬¸ìž¥\n",
    "        \n",
    "    Returns:\n",
    "        int: ì˜ˆì¸¡ëœ ê°ì„± ë¼ë²¨ (1 ë˜ëŠ” -1)\n",
    "    \"\"\"\n",
    "    embedding = model.encode([sentence])\n",
    "    pred = clf.predict(embedding)\n",
    "    return int(pred[0])\n",
    "\n",
    "# â”€â”€ 9. í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ â”€â”€\n",
    "test_sentence = \"ì´ ì œí’ˆì€ ì •ë§ íš¨ìœ¨ì ì´ê³  í™˜ê²½ì—ë„ ì¢‹ì•„ìš”.\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë¬¸ìž¥:\", test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab31b0f-5ebe-46d1-b0f3-b4bd2248ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "ìµœì ì˜ íŒŒë¼ë¯¸í„°: {'C': 10}\n",
      "ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœê³  ì •í™•ë„: 72.73%\n",
      "== SVM ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„°) ==\n",
      "ì „ì²´ ì •í™•ë„: 96.36%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      1.00      0.97        35\n",
      "           1       1.00      0.90      0.95        20\n",
      "\n",
      "    accuracy                           0.96        55\n",
      "   macro avg       0.97      0.95      0.96        55\n",
      "weighted avg       0.97      0.96      0.96        55\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬:\n",
      "[[35  0]\n",
      " [ 2 18]]\n",
      "í…ŒìŠ¤íŠ¸ ë¬¸ìž¥: ëŒ€ì¤‘êµí†µì„ íƒ„ë‹¤\n",
      "ì˜ˆì¸¡ ê²°ê³¼: -1\n",
      "ì ìš© ì™„ë£Œ: descriptions_with_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# â”€â”€ 1. ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ + ì¶”ê°€) â”€â”€\n",
    "positive_words_100 = [\n",
    "    \"ì ˆì•½\", \"íš¨ìœ¨\", \"ì¹œí™˜\", \"ìž¬í™œ\", \"ë³´ì¡´\", \"ì ˆê°\", \"ìµœì†Œ\", \"ì§€ì†\", \"ì ˆì „\", \"ì €íƒ„\",\n",
    "    \"ê°œì„ \", \"ì •í™”\", \"ìˆœí™˜\", \"ìž¬ìƒ\", \"ë…¹ìƒ‰\", \"ì²­ì •\", \"ì‚°ë¦¼\", \"í•´ì–‘\", \"ìžì—°\", \"ì¤‘ë¦½\",\n",
    "    \"ë‹¤ì–‘\", \"ì—ì½”\", \"í˜ì‹ \", \"ì‹ ìž¬\", \"ë…¹ì„±\", \"íšŒë³µ\", \"ë„ì‹œ\", \"ê²½ì˜\", \"íˆ¬ìž\", \"ì •ì±…\",\n",
    "    \"ìžì›\", \"ì†Œë¹„\", \"ì˜ì‹\", \"ìŠ¤ë§ˆíŠ¸\", \"ë…¹ê¸°\", \"ë¬¼ì ˆ\", \"ì¹œë†\", \"ìžë¦½\", \"ì‹¤ì²œ\", \"í–‰ë™\",\n",
    "    \"ì§€êµ¬\", \"í™˜ê¸°\", \"í™˜ì›\", \"ê°ì†Œ\", \"ì¡°ì ˆ\", \"ì—ë„ˆ\", \"ì‚°ì—…\", \"ë…¹ì‚°\", \"ë°”ëžŒ\", \"íƒœì–‘\",\n",
    "    \"ìˆ˜ë ¥\", \"í’ë ¥\", \"ì „í™˜\", \"ì €ë°°\", \"ë…¹ì „\", \"ì¹œê±´\", \"ì¹œì†Œ\", \"ì¹œí™”\", \"ë…¹ì—…\", \"ì‹ ê¸°\",\n",
    "    \"ì‚°ë³´\", \"í•´ë³´\", \"ì¹œì—…\", \"ì „ë ¥\", \"ë…¹ìœ¨\", \"ë…¹ì›\", \"ì¹œë„\", \"ë…¹ì§€\", \"ì²­ì›\", \"ìž¬ìˆœ\",\n",
    "    \"ì¹œìƒ\", \"í™˜ì‚°\", \"íƒ„ê°\", \"ë…¹ì‹¤\", \"ë…¹í™˜\", \"ì—ë…¹\", \"ì‹ í™˜\", \"ìží™˜\", \"ì²­í™˜\", \"ë…¹ì°½\",\n",
    "    \"ì—ì¹œ\", \"íš¨ë³´\", \"ì—ë³´\", \"ì‚°ì²­\", \"ë…¹ì²­\", \"ì—ì‹ \", \"ìžê²½\", \"ì¹œìž\", \"ì‚°ì§€\", \"í•´ì§€\",\n",
    "    \"ìžì§€\", \"ì—ê²½\", \"ì—ì •\", \"ìž¬ê°œ\", \"ë…¹ê°œ\", \"ìˆœë³´\", \"ì—ìˆœ\", \"ì¹œí•©\", \"ìží•©\", \"ë…¹í•©\"\n",
    "]\n",
    "negative_words_100 = [\n",
    "    \"ë‚­ë¹„\", \"ê³¼ë‹¤\", \"ë¬´ë¶„\", \"ìƒì‹œ\", \"ë¶ˆí•„\", \"ê³¼ë„\", \"ê³¼ì†Œ\", \"ì˜¤ì—¼\", \"íê¸°\", \"íŒŒê´´\",\n",
    "    \"ë°°ì¶œ\", \"ë…ì„±\", \"ë¶€ì \", \"ë¹„íš¨\", \"ë¬´ì ˆ\", \"ìžë‚­\", \"ë‚¨ìš©\", \"ë¬´ìš©\", \"ì†ŒìŒ\", \"ë¯¸ì„¸\",\n",
    "    \"í™”ì„\", \"ë¹„ìž¬\", \"ë…ë¬¼\", \"ì˜¨ì‹¤\", \"ê³¼ì—´\", \"ë¶ˆê· \", \"ë¶ˆì•ˆ\", \"íë¬¼\", \"ì—ë‚­\", \"ë¬´ê³„\",\n",
    "    \"íŒŒì \", \"í•´ë¡œ\", \"ìœ„í˜‘\", \"ë¶ˆì•ˆ\", \"ë¶€ì‹¤\", \"ë¶€ì •\", \"ìží˜•\", \"ë¶€ì‹¤ìš´\", \"ì˜¤ë‚¨\", \"ë¬´íš¨\",\n",
    "    \"ë…ë°°\", \"íì¦\", \"ì†Œê³¼\", \"ë¶ˆì¹œ\", \"ë¹„ë„\", \"ìœ„í—˜\", \"ë‚­ì„±\", \"ë¹„í•©\", \"ëŒ€ì˜¤\", \"ìˆ˜ì˜¤\",\n",
    "    \"í† ì˜¤\", \"ì†Œê³µ\", \"ì˜¤ë¬¼\", \"í•´ì˜¤\", \"ê¸°ì•…\", \"ë¶ˆíˆ¬\", \"ì‚°í\", \"ì‚°íŒŒ\", \"ìžíŒŒ\", \"ëŒ€ì§ˆ\",\n",
    "    \"ìžë‚¨\", \"í™”ì¶œ\", \"ìœ í•´\", \"íìˆ˜\", \"ì•…ì·¨\", \"ê¸°ìœ \", \"ìƒíŒŒ\", \"í”¼í•´\", \"ì“°ë²”\", \"ë¯¸í­\",\n",
    "    \"ë…í\", \"ë¶ˆë²•\", \"ë¶ˆê´‘\", \"ë¹„ìœ„\", \"ë¬´ì±…\", \"íë¶€\", \"ì˜¨ê³¼\", \"ì˜¨ë‚œ\", \"ê³µí•´\", \"ê·œë¯¸\",\n",
    "    \"í”¼ì¦\", \"ìž¬ë¶ˆ\", \"ë¬´ì¶”\", \"ì“°ë¬¸\", \"ë„ì—´\", \"ì˜¨ë³€\", \"ìœ„ê¸°\", \"ìƒí–‰\", \"ë¶ˆí•©\", \"ë¹„ë¶ˆ\",\n",
    "    \"ë¶€í‡´\", \"íì•…\", \"ë¶ˆí\", \"ë¹„ë‚­\", \"ë¹„ì˜¤\", \"ê³¼ë°°\", \"ë¶€ì˜¤\", \"ë¹„í\", \"ì§€ì˜¤\", \"ì‚°ì˜¤\"\n",
    "]\n",
    "\n",
    "# ì¶”ê°€ ë‹¨ì–´\n",
    "additional_positive_words = [\"ì‹ ë¢°\", \"í’ˆì§ˆ\"]\n",
    "additional_negative_words = [\"ë¶ˆë§Œ\", \"ë¬¸ì œ\"]\n",
    "\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "# â”€â”€ 2. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ â”€â”€\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # êµ¬ë‘ì  ì œê±°\n",
    "    return text\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë‚´ ê¸ì •/ë¶€ì • ë‹¨ì–´ ë“±ìž¥ íšŸìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ ë°˜í™˜\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# â”€â”€ 3. CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ â”€â”€\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df.columns)\n",
    "\n",
    "# description ì»¬ëŸ¼ ì „ì²˜ë¦¬ ë° lexicon ì ìˆ˜ ê³„ì‚°\n",
    "df[\"description_clean\"] = df[\"description\"].apply(clean_text)\n",
    "df[\"lex_score\"] = df[\"description_clean\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# â”€â”€ 4. SentenceTransformer ìž„ë² ë”© ê³„ì‚° â”€â”€\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ëª¨ë“  ë¬¸ìž¥ì— ëŒ€í•´ ìž„ë² ë”© ê³„ì‚° (ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)\n",
    "descriptions = df[\"description_clean\"].tolist()\n",
    "embeddings = model.encode(descriptions)\n",
    "\n",
    "# â”€â”€ 5. íŠ¹ì§• ê²°í•©: ìž„ë² ë”© + lexicon ì ìˆ˜ â”€â”€\n",
    "# lexicon ì ìˆ˜ëŠ” ìŠ¤ì¹¼ë¼ì´ë¯€ë¡œ ì°¨ì› ë§žì¶”ê¸° ìœ„í•´ reshape\n",
    "lex_scores = df[\"lex_score\"].values.reshape(-1, 1)\n",
    "# ì˜ˆ: ìž„ë² ë”© ì°¨ì›ì´ (N, D)ì¼ ë•Œ ìµœì¢… íŠ¹ì§• ì°¨ì›ì€ (N, D+1)\n",
    "X_combined = np.hstack([embeddings, lex_scores])\n",
    "y = df[\"PN\"].values  # ì •ë‹µ ë¼ë²¨\n",
    "\n",
    "# â”€â”€ 6. SVM ë¶„ë¥˜ê¸° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (GridSearchCV) â”€â”€\n",
    "svm_clf = SVC(kernel='linear', probability=True)\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_combined, y)\n",
    "\n",
    "print(\"ìµœì ì˜ íŒŒë¼ë¯¸í„°:\", grid_search.best_params_)\n",
    "print(\"ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœê³  ì •í™•ë„: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "# ìµœì ì˜ ë¶„ë¥˜ê¸°ë¡œ ìµœì¢… í•™ìŠµ\n",
    "svm_clf_final = grid_search.best_estimator_\n",
    "\n",
    "# â”€â”€ 7. í•™ìŠµ ë°ì´í„° í‰ê°€ â”€â”€\n",
    "pred_train = svm_clf_final.predict(X_combined)\n",
    "print(\"== SVM ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„°) ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(y, pred_train) * 100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y, pred_train))\n",
    "print(\"í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(y, pred_train))\n",
    "\n",
    "# â”€â”€ 8. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ â”€â”€\n",
    "def predict_sentiment(sentence, use_combined=True):\n",
    "    sentence_clean = clean_text(sentence)\n",
    "    emb = model.encode([sentence_clean])\n",
    "    if use_combined:\n",
    "        lex_score = np.array([[lexicon_score(sentence_clean, positive_words, negative_words)]])\n",
    "        feature = np.hstack([emb, lex_score])\n",
    "        pred = svm_clf_final.predict(feature)\n",
    "    else:\n",
    "        pred = svm_clf_final.predict(emb)\n",
    "    return int(pred[0])\n",
    "\n",
    "# â”€â”€ 9. í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ â”€â”€\n",
    "test_sentence = \"ëŒ€ì¤‘êµí†µì„ íƒ„ë‹¤\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë¬¸ìž¥:\", test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)\n",
    "\n",
    "# ê²°ê³¼ CSV íŒŒì¼ë¡œ ì €ìž¥ (ì˜ˆ: descriptions_with_lexicon.csv)\n",
    "df.to_csv(\"./bulkdata/descriptions_with_lexicon.csv\", index=False)\n",
    "\n",
    "print(\"ì ìš© ì™„ë£Œ: descriptions_with_lexicon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2290c8ec-947c-46ec-bba3-c4d5df5654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "positive_words_100 = [\n",
    "    \"ì„¤ì •\",\n",
    "    \"í•„ìš”í•œ\",\n",
    "    \"ëª¨ì•„ì„œ\",\n",
    "    \"ëŒë¦°ë‹¤\",\n",
    "    \"ë½‘ëŠ”ë‹¤\",\n",
    "    \"ëšœê»‘ ë®ë‹¤\",\n",
    "    \"ë‚®ì¶”ë‹¤\",\n",
    "    \"ì ê²€\",\n",
    "    \"êµì²´í•œë‹¤\",\n",
    "    \"êº¼ë‘”ë‹¤\",\n",
    "    \"ìž ê·¼ë‹¤\",\n",
    "    \"ë‹¤íšŒìš©\",\n",
    "    \"ì¡°ë¦¬í•œë‹¤\",\n",
    "    \"ìžì „ê±°\",\n",
    "    \"ê±·ê¸°\",\n",
    "    \"ì–‘ë©´\",\n",
    "    \"ê°ì†Œ\",\n",
    "    \"ì¤„ì¸ë‹¤\",\n",
    "    \"ëŒ€ì¤‘êµí†µ\"\n",
    "]\n",
    "\n",
    "negative_words_100 = [\n",
    "    \"í˜•ê´‘ë“±\",\n",
    "    \"ìœ ì§€í•œë‹¤\",\n",
    "    \"ì „ê¸°ë°¥ì†¥\",\n",
    "    \"ë³´ì˜¨\",\n",
    "    \"í•˜ë£¨ ì¢…ì¼\",\n",
    "    \"ì¼œë‘”ë‹¤\",\n",
    "    \"ìŒì‹ë¬¼ ì°Œêº¼ê¸°\",\n",
    "    \"ì”»ì–´ë³´ë‚¸ë‹¤\",\n",
    "    \"ì„¸ì œ\",\n",
    "    \"ê±¸ë ˆ\",\n",
    "    \"íë¥´ëŠ” ë¬¼\",\n",
    "    \"í—¹êµ°ë‹¤\",\n",
    "    \"ë¬¼í‹°ìŠˆ\",\n",
    "    \"ê¸°ì €ê·€\",\n",
    "    \"ë³´ì¼ëŸ¬\",\n",
    "    \"í‹€ì–´ë†“ëŠ”ë‹¤\",\n",
    "    \"ì™¸ì¶œ\",\n",
    "    \"ê°€ìŠ¤ë ˆì¸ì§€\",\n",
    "    \"ë¶ˆê½ƒ\",\n",
    "    \"í¼ì§€ë‹¤\",\n",
    "    \"ë§¤ì¼\",\n",
    "    \"ëƒ‰ìž¥ê³  ë¬¸\",\n",
    "    \"ì—°ë‹¤\",\n",
    "    \"ì—ì–´ì»¨\",\n",
    "    \"ë‚®ê²Œ ìœ ì§€\",\n",
    "    \"ì»´í“¨í„°\",\n",
    "    \"í•­ìƒ ì¼œë‘”ë‹¤\",\n",
    "    \"ìµœëŒ€ ë¶ˆ\",\n",
    "    \"ë“ì¸ë‹¤\",\n",
    "    \"ê°€ìŠ¤\",\n",
    "    \"ì¼œë‘”\",\n",
    "    \"ì˜¤ëžœ ì‹œê°„\",\n",
    "    \"ë‚œë°©\",\n",
    "    \"ëª©ìš•íƒ•\",\n",
    "    \"ì±„ìš´ë‹¤\",\n",
    "    \"ë§Žì´ ì‚¬ìš©í•œë‹¤\",\n",
    "    \"ê³„ì† í‹€ì–´ë‘”ë‹¤\",\n",
    "    \"ì°½ë¬¸\",\n",
    "    \"ì—´ì–´ë‘”ë‹¤\",\n",
    "    \"ì§‘ ì „ì²´\",\n",
    "    \"ë‚œë°©í•œë‹¤\",\n",
    "    \"ë‚œë°©ê¸°\",\n",
    "    \"ìµœê³  ì˜¨ë„\",\n",
    "    \"ì„¤ì •í•œë‹¤\",\n",
    "    \"ì¼íšŒìš©\",\n",
    "    \"í”Œë¼ìŠ¤í‹±\",\n",
    "    \"í¬ìž¥\",\n",
    "    \"êµ¬ë§¤í•œë‹¤\",\n",
    "    \"ë¹„ë‹ë´‰ì§€\",\n",
    "    \"ë„‰ë„‰ížˆ ë§Œë“ ë‹¤\",\n",
    "    \"ìŒì‹\",\n",
    "    \"ë²„ë¦°ë‹¤\",\n",
    "    \"ë³´ê´€í•˜ì§€ ì•ŠëŠ”ë‹¤\",\n",
    "    \"ì°¨\",\n",
    "    \"ìžì£¼ ì´ìš©í•œë‹¤\",\n",
    "    \"ìžë™ì°¨\",\n",
    "    \"ì´ìš©í•œë‹¤\",\n",
    "    \"ì´ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤\",\n",
    "    \"ì¢…ì´\",\n",
    "    \"ì¼íšŒìš©\",\n",
    "    \"í•œìª½ ë©´\",\n",
    "    \"ì¸ì‡„í•œë‹¤\",\n",
    "    \"ì£¼ì°¨ ê³µê°„\",\n",
    "    \"ë¶€ì¡±\",\n",
    "    \"ì‹¬í™”ì‹œí‚¨ë‹¤\",\n",
    "    \"ë„ë¡œ í™•ìž¥\",\n",
    "    \"ìœ ë°œí•œë‹¤\",\n",
    "    \"í™˜ê²½ íŒŒê´´\",\n",
    "    \"í™”ì„ ì—°ë£Œ\",\n",
    "    \"ì‚¬ìš©\",\n",
    "    \"ê°•ì œí•œë‹¤\",\n",
    "    \"ì§€êµ¬ ì˜¨ë‚œí™”\",\n",
    "    \"ê°€ì†í™”í•œë‹¤\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f4e4195-ddb8-43d0-9559-15f7df5e576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ì»¬ëŸ¼: Index(['description', 'score', 'PN'], dtype='object')\n",
      "ìµœì ì˜ íŒŒë¼ë¯¸í„°: {'C': 1}\n",
      "ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœê³  ì •í™•ë„: 89.09%\n",
      "== SVM ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„°) ==\n",
      "ì „ì²´ ì •í™•ë„: 90.91%\n",
      "ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.94      0.93        35\n",
      "           1       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.91        55\n",
      "   macro avg       0.91      0.90      0.90        55\n",
      "weighted avg       0.91      0.91      0.91        55\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬:\n",
      "[[33  2]\n",
      " [ 3 17]]\n",
      "í…ŒìŠ¤íŠ¸ ë¬¸ìž¥: ëŒ€ì¤‘êµí†µì„ íƒ„ë‹¤\n",
      "ì˜ˆì¸¡ ê²°ê³¼: 1\n",
      "ì ìš© ì™„ë£Œ: descriptions_with_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ê°€ ë‹¨ì–´\n",
    "additional_positive_words = [\"ì‹ ë¢°\", \"í’ˆì§ˆ\"]\n",
    "additional_negative_words = [\"ë¶ˆë§Œ\", \"ë¬¸ì œ\"]\n",
    "\n",
    "positive_words = positive_words_100 + additional_positive_words\n",
    "negative_words = negative_words_100 + additional_negative_words\n",
    "\n",
    "# â”€â”€ 2. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ â”€â”€\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # êµ¬ë‘ì  ì œê±°\n",
    "    return text\n",
    "\n",
    "def lexicon_score(text, pos_list, neg_list):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë‚´ ê¸ì •/ë¶€ì • ë‹¨ì–´ ë“±ìž¥ íšŸìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ ë°˜í™˜\"\"\"\n",
    "    pos_count = sum(1 for word in pos_list if word in text)\n",
    "    neg_count = sum(1 for word in neg_list if word in text)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "# â”€â”€ 3. CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ â”€â”€\n",
    "df = pd.read_csv(\"./bulkdata/descriptions_with_PN.csv\")\n",
    "print(\"ë°ì´í„°ì…‹ ì»¬ëŸ¼:\", df.columns)\n",
    "\n",
    "# description ì»¬ëŸ¼ ì „ì²˜ë¦¬ ë° lexicon ì ìˆ˜ ê³„ì‚°\n",
    "df[\"description_clean\"] = df[\"description\"].apply(clean_text)\n",
    "df[\"lex_score\"] = df[\"description_clean\"].apply(lambda x: lexicon_score(x, positive_words, negative_words))\n",
    "\n",
    "# â”€â”€ 4. SentenceTransformer ìž„ë² ë”© ê³„ì‚° â”€â”€\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "# ëª¨ë“  ë¬¸ìž¥ì— ëŒ€í•´ ìž„ë² ë”© ê³„ì‚° (ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)\n",
    "descriptions = df[\"description_clean\"].tolist()\n",
    "embeddings = model.encode(descriptions)\n",
    "\n",
    "# â”€â”€ 5. íŠ¹ì§• ê²°í•©: ìž„ë² ë”© + lexicon ì ìˆ˜ â”€â”€\n",
    "# lexicon ì ìˆ˜ëŠ” ìŠ¤ì¹¼ë¼ì´ë¯€ë¡œ ì°¨ì› ë§žì¶”ê¸° ìœ„í•´ reshape\n",
    "lex_scores = df[\"lex_score\"].values.reshape(-1, 1)\n",
    "# ì˜ˆ: ìž„ë² ë”© ì°¨ì›ì´ (N, D)ì¼ ë•Œ ìµœì¢… íŠ¹ì§• ì°¨ì›ì€ (N, D+1)\n",
    "X_combined = np.hstack([embeddings, lex_scores])\n",
    "y = df[\"PN\"].values  # ì •ë‹µ ë¼ë²¨\n",
    "\n",
    "# â”€â”€ 6. SVM ë¶„ë¥˜ê¸° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (GridSearchCV) â”€â”€\n",
    "svm_clf = SVC(kernel='linear', probability=True)\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_combined, y)\n",
    "\n",
    "print(\"ìµœì ì˜ íŒŒë¼ë¯¸í„°:\", grid_search.best_params_)\n",
    "print(\"ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœê³  ì •í™•ë„: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "# ìµœì ì˜ ë¶„ë¥˜ê¸°ë¡œ ìµœì¢… í•™ìŠµ\n",
    "svm_clf_final = grid_search.best_estimator_\n",
    "\n",
    "# â”€â”€ 7. í•™ìŠµ ë°ì´í„° í‰ê°€ â”€â”€\n",
    "pred_train = svm_clf_final.predict(X_combined)\n",
    "print(\"== SVM ë¶„ë¥˜ê¸° í‰ê°€ (í•™ìŠµ ë°ì´í„°) ==\")\n",
    "print(\"ì „ì²´ ì •í™•ë„: {:.2f}%\".format(accuracy_score(y, pred_train) * 100))\n",
    "print(\"ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y, pred_train))\n",
    "print(\"í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(y, pred_train))\n",
    "\n",
    "# â”€â”€ 8. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ â”€â”€\n",
    "def predict_sentiment(sentence, use_combined=True):\n",
    "    sentence_clean = clean_text(sentence)\n",
    "    emb = model.encode([sentence_clean])\n",
    "    if use_combined:\n",
    "        lex_score = np.array([[lexicon_score(sentence_clean, positive_words, negative_words)]])\n",
    "        feature = np.hstack([emb, lex_score])\n",
    "        pred = svm_clf_final.predict(feature)\n",
    "    else:\n",
    "        pred = svm_clf_final.predict(emb)\n",
    "    return int(pred[0])\n",
    "\n",
    "# â”€â”€ 9. í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ â”€â”€\n",
    "test_sentence = \"ëŒ€ì¤‘êµí†µì„ íƒ„ë‹¤\"\n",
    "result = predict_sentiment(test_sentence)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë¬¸ìž¥:\", test_sentence)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", result)\n",
    "\n",
    "# ê²°ê³¼ CSV íŒŒì¼ë¡œ ì €ìž¥ (ì˜ˆ: descriptions_with_lexicon.csv)\n",
    "df.to_csv(\"./bulkdata/descriptions_with_lexicon.csv\", index=False)\n",
    "\n",
    "print(\"ì ìš© ì™„ë£Œ: descriptions_with_lexicon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc875f-297c-4405-a5d7-6ded60c6642c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
